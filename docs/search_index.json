[["index.html", "Data Management for Clinical Researchers Preface", " Data Management for Clinical Researchers Albert Cobos Preface This is an introductory book on data management for clinical researchers. The ability to manage data is a fundamental skill for data analysts, because only rarely the initial state of a dataset is adequate to perform the desired analysis. Rather, preparation tasks are usually needed to transform the data so that analysis functions can be used. In fact, these tasks tend to be more complex, more prone to errors, and much more time consuming, than the analysis itself. All data management tasks can be accomplished using base R. However, several R packages make them easier. Among these, packages dplyr, tidyr, stingr and forcats will be used in this book. These (and other) packages form a collection known as the tidyverse, which is integrated in a package of the same name (tidyverse). Once installed, you can load it with: library(tidyverse) And that’s all you need to start! "],["working-with-dataframes.html", "1 Working with dataframes 1.1 Subsetting dataframes 1.2 Reshaping dataframes 1.3 Summarising rows of a dataframe 1.4 Combining dataframes Resources Exercises", " 1 Working with dataframes Dataframes are the most common object used to store data in R, and many data analysis functions expect a dataframe as input. Frequently however, dataframes will need to undergo certain transformations, so that they satisfy the structure required by analysis functions. These transformations may involve subsetting rows or columns, aggregating rows, reshaping or combining dataframes. Although base R includes operators and functions allowing to perfom these and other dataframe operations, packages dplyr and tidyr make them easier hand have some additional advantages, such as allowing to chain operations with the pipe operator (%&gt;%). In this chapter, we will see how to perform all the above mentioned transformations in turn, using functions from the dplyr and tidyr packages. However, because these two packages are part of the tidyverse, you do not need to load them (provided you have loaded tidyverse!). library(tidyverse) For illustrative purposes, we will use data from a random sample of 500 patients participating in the DISEHTAE study on the diagnosis, follow-up and control of arterial hypertensin in Spain. This was a cross-sectional nationwide study, with external auditing performed in 7802 hypertensive subjects who had attended one of 107 primary care centers from 14 regions in Spain during 2003. Among other variables, blood pressure (BP) measurements documented in each patient’s clinical record were collected for a maximum of six follow-up visits in a natural year. Table 1.1 lists the variables contained in the data file. Download the DISETHAE data Table 1.1: Variables in the dataset variable description coding pid patient identification number data_xtract_dt data extraction date dd/mm/yyyy region region in Spain 1=Andalucía, 2=Aragón, 3=Asturias, 4=Baleares, 5=Canarias, 6=Cantabria, 7=Castilla-La Mancha, 8=Castilla-León, 9=Catalunya, 10=Extremadura, 11=Galicia, 12=La Rioja, 13=Madrid, 14=Murcia, 15=Navarra, 16=País Vasco, 17=Valencia age age (years) at study start date (01.Jan.2003) sex patient’s sex 1=male, 2=female ah_dx_dt date of diagnosis of arterial hypertension dd/mm/yyyy sbp_v1 systolic blood pressure, visit 1 (mmHg) dbp_v1 diastolic blood pressure, visit 1 (mmHg) sbp_v2 same, visit 2 dbp_v2 same, visit 2 sbp_v3 same, visit 3 dbp_v3 same, visit 3 sbp_v4 same, visit 4 dbp_v4 same, visit 4 sbp_v5 same, visit 5 dbp_v5 same, visit 5 sbp_v6 same, visit 6 dbp_v6 same, visit 6 glucose blood glucose concentraton (mg/dl) dx_dm diagnosed with diabetes mellitus 1=yes, 2=no total_c total cholesterol (mg/dl) hdl_c HDL-choleterol (mg/dl) ldl_c LDL-choleterol (mg/dl) trigly triglycerides (mg/dl) dx_dyslip diagnosed with dyslipidemia 1=yes, 2=no smoke smoking status 1=current smoker, 2=never smoker, 3=ex-smoker, 4=unknown weight body weight (kg) height body height (meters) dx_lvh diagnosed with left ventricula hypertrophy 1=yes, 2=no cv_risk_record cardiovascular risk assessment recorded 1=yes, 2=no, 3=not applicable (previous cardiovascular event) creatinine creatinine concentration in blood (mg/dl) lmr lifestyle modification recommendation 1=yes, 2=no bb treated with beta-blockers 1=yes, 2=no diur treated with diuretics 1=yes, 2=no acei treated with angiotensin converting enzyme inhibitors 1=yes, 2=no arb treated with angiotensin receptor blockers 1=yes, 2=no cbb treated with calcium channel blockers 1=yes, 2=no ab treated with alpha_blockers 1=yes, 2=no other treated with other antihypertensive drugs 1=yes, 2=no The following script reads the data with function import() from package rio. The argument which specifies the spreadsheet to be read (data), since the MS Excel file has more than one spreadsheet. The resulting dataframe is saved as ah: ah &lt;- rio::import(&quot;./data/hta.xlsx&quot;, which = &quot;data&quot;) 1.1 Subsetting dataframes Subsetting dataframes is a very common task. When a dataframe has many variables (columns), we may want to subset the variables needed for an analysis as outlined in table 1.2. In other instances we may want to restrict our analysis to a particular subset of observations (rows), as depicted in table 1.3. Table 1.2: Subset variables (columns) x1 x2 x3 x4 x5 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 x2 x3 x5 1 1 1 2 2 2 3 3 3 4 4 4 Table 1.3: Subset observations (rows) x1 x2 x3 x4 x5 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 x1 x2 x3 x4 x5 2 2 2 2 2 3 3 3 3 3 1.1.1 Subsetting variables (columns) The select() function in dplyr allows to select variables from a dataframe. The first argument to this function must be the dataframe we want to subset. After it, we may simply name the variables we want to retain. In the example below we chain the head() function with the pipe operator %&gt;% to limit the output to the first 6 rows: select(ah, pid, region, age, sex) %&gt;% head() pid region age sex 1 11 6 52 1 2 15 6 57 2 3 20 1 62 1 4 24 10 69 2 5 33 10 70 2 6 37 9 64 2 The variables to select may be specified one by one, comma separated, as in the above example. A range of consecutive variables can be indicated using the colon (:). Also, we can use the minus sign - to indicate variables to exclude: select(ah, pid, pid:sex, -data_xtract_dt) %&gt;% head() pid region age sex 1 11 6 52 1 2 15 6 57 2 3 20 1 62 1 4 24 10 69 2 5 33 10 70 2 6 37 9 64 2 Some helper functions are available to facilitate the subsetting based on the variable names, which is very handy if variables are named in a consistent way. In dataframe ah, all date variables are suffixed with the string “dt”. Similarly, the name of all diagnostic variables is prefixed with “dx”. Suppose we want to select all variables containing the string “dx”. This can be easily done with the helper function contains(): select(ah, contains(&quot;dx&quot;)) %&gt;% # all variables containing &quot;dx&quot; head() ah_dx_dt dx_dm dx_dyslip dx_lvh 1 1998-01-01 2 2 2 2 1997-01-01 2 2 2 3 2003-04-10 2 2 2 4 1993-01-01 2 1 2 5 1994-01-01 2 1 2 6 1995-04-07 1 2 2 Helper functions starts_with() and ends_with() allow to select variables preffixed or suffixed with the specified string. These are useful to select all diagnostic or all date variables, respectively: select(ah, starts_with(&quot;dx&quot;)) %&gt;% # all diagnostic vars head() dx_dm dx_dyslip dx_lvh 1 2 2 2 2 2 2 2 3 2 2 2 4 2 1 2 5 2 1 2 6 1 2 2 select(ah, ends_with(&quot;dt&quot;)) %&gt;% # all dates head() data_xtract_dt ah_dx_dt 1 2003-03-15 1998-01-01 2 2003-03-15 1997-01-01 3 2003-05-13 2003-04-10 4 2003-05-27 1993-01-01 5 2003-05-28 1994-01-01 6 2003-06-14 1995-04-07 When some variables are measured more than once, it is common to number repetitions by suffixing a repetion number, as is the case of blood pressure measurements (sbp_v1 to sbp_v6 and dbp_v1 to dbp_v6). The helper function num_range() allows to select variables so named, by indicating the constant part of the variable names as the first argument, and the numeric range of repetitions to select as the second argument: select(ah, num_range(&quot;sbp_v&quot;, 1:3)) %&gt;% # SBP repetitions 1 to 3 head() sbp_v1 sbp_v2 sbp_v3 1 130 140 NA 2 130 120 120 3 130 120 140 4 150 160 140 5 140 145 140 6 159 160 159 select(ah, num_range(&quot;dbp_v&quot;, 2:5)) %&gt;% # DBP repetitions 2 to 5 head() dbp_v2 dbp_v3 dbp_v4 dbp_v5 1 80 NA NA NA 2 80 90 90 NA 3 80 80 80 80 4 85 95 85 90 5 85 80 85 80 6 80 75 80 79 If the names of the variables to be selected are contained in a character vector, the helper function one_of() is handy: demovars &lt;- c(&quot;pid&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;region&quot;) demo &lt;- select(ah, one_of(demovars)) head(demo) pid age sex region 1 11 52 1 6 2 15 57 2 6 3 20 62 1 1 4 24 69 2 10 5 33 70 2 10 6 37 64 2 9 1.1.2 Subsetting observations (rows) The rows of a dataframe can be subsetted in several ways: by position (i.e., row number), at random, or according to some logical condition. When working with dataframes having a very large number of observations, it is convenient to use a smaller subset to start programming the analysis, so that the execution doesn’t take too long. Only when we have verified that the R code works well, we will run it on the whole dataframe. To this end, the slice() function is useful to select a range of rows: slice(demo, 1:3) # rows 1 to 3 pid age sex region 1 11 52 1 6 2 15 57 2 6 3 20 62 1 1 Functions slice_min() and slice_max() allow the selection of observartions having the highest or lowest values in a variable: slice_min(demo, age, n=3) # the 3 youngest cases pid age sex region 1 1308 32 1 7 2 398 33 1 5 3 1420 34 2 5 slice_max(demo, age, n=5) # the 5 oldest cases pid age sex region 1 562 99 2 3 2 3551 91 2 17 3 1311 89 2 13 4 981 88 2 5 5 1847 87 1 9 6 3355 87 2 1 7 5080 87 2 17 If we want to take a random sample of observations, sample_n() allows to specify the size of the sample we want to draw from the dataframe. When using this (or any other function using the random number generator), we need to set the seed if we want the result of the function to be reproducible (i.e, to give the same result every time we execute the code). This will select five cases at random from dataframe demo: set.seed(1) # for reproducibility sample_n(demo, size = 5) # actual sampling of 5 cases from demo pid age sex region 1 5248 61 2 6 2 2305 80 1 9 3 1590 73 2 9 4 6778 62 1 14 5 7429 59 2 9 Last, observations may be selected based on a logical condition, that is, something that resolves to a logical value (TRUE or FALSE) for each row. Conditions are specified by means of relational operators (&gt;, &gt;=, &lt;, &lt;=, ==, and !=), and may be combined with logical operators (&amp;, |, !). The following example retains all females aged more than 65 (but then we use head() to limit the length of the output): filter(demo, sex == 2 &amp; age &gt; 65) %&gt;% head() pid age sex region 1 24 69 2 10 2 33 70 2 10 3 114 73 2 5 4 155 66 2 5 5 169 72 2 5 6 193 78 2 5 The helper function between()is useful to specify ranges for numeric variables: filter(demo, sex == 2 &amp; between(age, 40, 65)) %&gt;% head() pid age sex region 1 15 57 2 6 2 37 64 2 9 3 50 55 2 2 4 176 60 2 5 5 177 54 2 5 6 224 62 2 5 Also, functions resolving to a logical value can be used, for instance to select cases with missing values (here we do not use head() to limit the output, because the number of rows filtered in limited) … filter(demo, is.na(sex)) pid age sex region 1 397 42 NA 5 2 2651 52 NA 9 3 3779 46 NA 13 4 4221 77 NA 17 5 4650 70 NA 2 6 5087 71 NA 17 7 6195 44 NA 8 8 6232 38 NA 14 9 7154 65 NA 1 … or not being missing : filter(demo, !is.na(sex)) %&gt;% head() pid age sex region 1 11 52 1 6 2 15 57 2 6 3 20 62 1 1 4 24 69 2 10 5 33 70 2 10 6 37 64 2 9 1.2 Reshaping dataframes Given a set of data, there are always several possible ways to arrange them in a dataframe. Figure 1.4 illustrates two different ways of arranging three variables (var_1, var_2 and var_3) evaluated in three individuals (id). On the left arrangement, sometines called wide format, each individual is a row and each variable is a column of the dataframe. On the right arrangement (called long format), each individual takes one row per variable and therefore three rows overall. Table 1.4: Two ways to structure the same data wide format id var_1 var_2 var_3 1 a b c 2 d e f 3 g h i long format id variable value 1 var_1 a 1 var_2 b 1 var_3 c 2 var_1 d 2 var_2 e 2 var_3 f 3 var_1 g 3 var_2 h 3 var_3 i Each of these arrangements has its own advantages and shortcomings, and we will choose one or the other as convenient. For instance, the wide format may be required by some analysis functions, while a graphic function may require the long format. Therefore, it is crucial to know how to reshape the data in a dataframe. The tidyr package has a couple of functions to reshape dataframes: pivot_long(): to convert a (wide) dataframe to long format pivot_wide(): tp convert a (long) dataframe to wide format To illustrate the use of these two functions, we select the patient identifier (pid) and all blood pressure measurements from dataframe ah, we save the resulting dataframe as bp, and we show its first six rows: bp_wide &lt;- select(ah, pid, contains(&quot;bp&quot;)) head(bp_wide) pid sbp_v1 dbp_v1 sbp_v2 dbp_v2 sbp_v3 dbp_v3 sbp_v4 dbp_v4 sbp_v5 dbp_v5 1 11 130 80 140 80 NA NA NA NA NA NA 2 15 130 80 120 80 120 90 130 90 NA NA 3 20 130 85 120 80 140 80 130 80 120 80 4 24 150 85 160 85 140 95 140 85 145 90 5 33 140 80 145 85 140 80 130 85 140 80 6 37 159 77 160 80 159 75 145 80 146 79 sbp_v6 dbp_v6 1 NA NA 2 NA NA 3 NA NA 4 140 90 5 140 75 6 146 70 1.2.1 Wide to long To use the pivot_longer() function, we need to specify the dataframe we want to reshape as the first argument, and then indicate the variables we want to verticalize; this can be done by listing them comma separated, or by indicating a range of variables, as done below. In addition, we may optionally provide a name for the (new) variable that will contain the names of verticalized variables: pivot_longer(bp_wide, sbp_v1:dbp_v6, names_to = &quot;variable&quot;) %&gt;% head(15) # A tibble: 15 × 3 pid variable value &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 11 sbp_v1 130 2 11 dbp_v1 80 3 11 sbp_v2 140 4 11 dbp_v2 80 5 11 sbp_v3 NA 6 11 dbp_v3 NA 7 11 sbp_v4 NA 8 11 dbp_v4 NA 9 11 sbp_v5 NA 10 11 dbp_v5 NA 11 11 sbp_v6 NA 12 11 dbp_v6 NA 13 15 sbp_v1 130 14 15 dbp_v1 80 15 15 sbp_v2 120 Because not all patients had their blood pressure measured in all six visits, many rows in the resulting dataframe are useless (e.g., sbp_v3 to dbp_v6 for patient 11). These can be ommited using function na.omit(): bp_long &lt;- pivot_longer(bp_wide, sbp_v1:dbp_v6, names_to = &quot;variable&quot;) %&gt;% na.omit() bp_long # A tibble: 3,038 × 3 pid variable value &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 11 sbp_v1 130 2 11 dbp_v1 80 3 11 sbp_v2 140 4 11 dbp_v2 80 5 15 sbp_v1 130 6 15 dbp_v1 80 7 15 sbp_v2 120 8 15 dbp_v2 80 9 15 sbp_v3 120 10 15 dbp_v3 90 # … with 3,028 more rows 1.2.2 Long to wide To use the pivot_wider() function, we need to specify the dataframe we want to reshape as the first argument, and then indicate the variables we want to horizontalize; this is done by identifying the names and the vaues of the variables to be horizontalized in arguments names_from = and values_from =, respectively: pivot_wider(bp_long, names_from = variable, values_from = value) # A tibble: 392 × 13 pid sbp_v1 dbp_v1 sbp_v2 dbp_v2 sbp_v3 dbp_v3 sbp_v4 dbp_v4 sbp_v5 dbp_v5 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 130 80 140 80 NA NA NA NA NA NA 2 15 130 80 120 80 120 90 130 90 NA NA 3 20 130 85 120 80 140 80 130 80 120 80 4 24 150 85 160 85 140 95 140 85 145 90 5 33 140 80 145 85 140 80 130 85 140 80 6 37 159 77 160 80 159 75 145 80 146 79 7 83 145 85 150 90 NA NA NA NA NA NA 8 91 150 100 NA NA NA NA NA NA NA NA 9 114 140 70 125 80 NA NA NA NA NA NA 10 145 140 80 150 85 NA NA NA NA NA NA # … with 382 more rows, and 2 more variables: sbp_v6 &lt;dbl&gt;, dbp_v6 &lt;dbl&gt; 1.2.3 Example Suppose we want to determine whether or not BP was controlled at each follow_up visit, defining BP control as SBP &lt; 140 and DBP &lt; 90. To work this out using bp_wide, a new “control” variable should be created for each sbp_ dbp_ pair, so that six new variables would be added to the dataframe, one for each of the six follow-up visits; Using bp_long is not an option, since SBP and DBP values for the same patient and visit are in different rows, and therefore the condition for control cannot be assessed, since it involves both SBP and DBP. A better structure for BP data would be one having one row for each patient visit, and SBP and DBP values in the same row. The following produces a dataframe with such structure, starting with dataframe ah, and chaining the following operations: select all variables containing the string “bp” (i.e., all BP variables). convert to long format, storing variable names in new variable variable. discard rows with a missing value. use function separate() to split the contents of variable into two new variables: measure (taking values sbp or dbp), and visit (taking values v1, v2, ...,v6`). convert to wide format, so that SBP and DBP values are horizontalized. bp &lt;- ah %&gt;% select(pid, contains(&quot;bp&quot;)) %&gt;% pivot_longer(sbp_v1:dbp_v6, names_to = &quot;variable&quot;) %&gt;% na.omit() %&gt;% separate(variable, into = c(&quot;measure&quot;, &quot;visit&quot;)) %&gt;% pivot_wider(names_from = measure, values_from = value) bp # A tibble: 1,519 × 4 pid visit sbp dbp &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 v1 130 80 2 11 v2 140 80 3 15 v1 130 80 4 15 v2 120 80 5 15 v3 120 90 6 15 v4 130 90 7 20 v1 130 85 8 20 v2 120 80 9 20 v3 140 80 10 20 v4 130 80 # … with 1,509 more rows This is possibly the most sensible structure to store BP data. Now it is very easy to compute the required variable informing on BP control: bp %&gt;% mutate(bp_control = ifelse(sbp &lt; 140 &amp; dbp &lt; 90, &quot;yes&quot;, &quot;no&quot;)) # A tibble: 1,519 × 5 pid visit sbp dbp bp_control &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 11 v1 130 80 yes 2 11 v2 140 80 no 3 15 v1 130 80 yes 4 15 v2 120 80 yes 5 15 v3 120 90 no 6 15 v4 130 90 no 7 20 v1 130 85 yes 8 20 v2 120 80 yes 9 20 v3 140 80 no 10 20 v4 130 80 yes # … with 1,509 more rows 1.3 Summarising rows of a dataframe The dplyr package has a summarise() function that can be used to compute summaries of columns of a dataframe. The first argument to this function is a dataframe, and further arguments are used to define the summaries we want to compute. For each summary, a name must be provided, and an appropriate function used to compute the desired summary of a column. An example of use of this function follows: summarise(ah, min_of_age = min(age), max_of_age = max(age), mean_of_age = mean(age), number_of_cases = n()) min_of_age max_of_age mean_of_age number_of_cases 1 32 99 65.59 500 Even though this may prove useful in some cases, it is usually more practical to compute summary statistics with functions such as summary() in base R, or favstats(), and tally() in package mosaic. What makes summarise() really powerful is its use in conjunction with the group_by() function, to compute summaries in groups of rows. 1.3.1 Summaries in grouped data The idea of summarising groups of rows is illustrated in figure 1.5. In this process, groups of rows having the same value in a grouping variable g are reduced to a single row containing summary values for the group. These summary values are computed by applying an appropriate function to combine or operate values of the different rows in a group. Table 1.5: Summarising grouped data grouped by g g x y 1 A 10 1 B 20 2 C 30 2 D 40 3 E 50 3 F 60 summarised by g g first_x pasted_x mean_y last_y 1 A A, B 15 20 2 C C, D 35 40 3 E E, F 55 60 For instance, consider the dataframe bp produced previously, having as many rows per patient as follow-up visits in which BP was measured (sbp and dbp). Suppose we want to compute the mean of all available BP measurements for each patient, as well as the number of such measurements. To do this, we need to: Group rows by patient (pid) with group_by(pid), Compute the required summaries with summarise(), providing a name for each, and Ungroup the dataframe with ungroup(). This is done by the following code: bp %&gt;% group_by(pid) %&gt;% # define the groups summarise(mean_sbp = mean(sbp), # computes the mean of sbp values mean_dbp = mean(dbp), # computes the mean of dbp values measurements = n()) %&gt;% # comptes the number of assessments (rows) ungroup() # A tibble: 392 × 4 pid mean_sbp mean_dbp measurements &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 11 135 80 2 2 15 125 85 4 3 20 128 81 5 4 24 146. 88.3 6 5 33 139. 80.8 6 6 37 152. 76.8 6 7 83 148. 87.5 2 8 91 150 100 1 9 114 132. 75 2 10 145 145 82.5 2 # … with 382 more rows The resulting dataframe has only one row per patient (pid), and the summaries computed are the remaining variables (mean_sbp, mean_dbp, and measurements). In computing summaries of a variable, we can use any function which is appropriate for the variable type. Variables sbp and dbp in bp are numeric variables, and we used the mean() function to compute their means for each patient. Other functions for numeric variables could have been used here, such as min(), max(), or median(). Similarly, character variables can be summarised using functions that are appropriate for characters. For example, consider the following dataframe containing the adverse events experienced by three patients: ae &lt;- data.frame(pid = c(1,2,2,2,3,3), adverse_event = c(&quot;Headache&quot;, &quot;Nausea&quot;, &quot;Vomiting&quot;, &quot;Abdominal cramps&quot;, &quot;Hip fracture&quot;, &quot;Anemia&quot;)) ae pid adverse_event 1 1 Headache 2 2 Nausea 3 2 Vomiting 4 2 Abdominal cramps 5 3 Hip fracture 6 3 Anemia Suppose we want to produce a report, showing all events for each patient. The following code uses the paste()function with option collapse = to write all events, as a single character value for each patient: ae %&gt;% group_by(pid) %&gt;% summarise(adverse_events = paste(adverse_event, collapse = &quot;, &quot;)) %&gt;% ungroup() # A tibble: 3 × 2 pid adverse_events &lt;dbl&gt; &lt;chr&gt; 1 1 Headache 2 2 Nausea, Vomiting, Abdominal cramps 3 3 Hip fracture, Anemia 1.3.2 Example 1 Suppose we want to compute how many anti-hypertensive drugs is taking each patient. The following script starts with dataframe ah, then selects the patient identified (pid) and the drug treatment variables (bb to other), then reshapes to a long format, and last changes values 2 to 0. The resulting dataframe is saved as drugs_longfor later use. drugs_long &lt;- ah %&gt;% select(pid, bb:other) %&gt;% pivot_longer(bb:other, names_to = &quot;drug&quot;) %&gt;% # verticalize drugs mutate(value = ifelse(value == 2, 0, value)) # recode values: 2 -&gt; 0 drugs_long # A tibble: 3,500 × 3 pid drug value &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 11 bb 1 2 11 diur 0 3 11 acei 0 4 11 arb 0 5 11 cbb 0 6 11 ab 0 7 11 other 0 8 15 bb 0 9 15 diur 0 10 15 acei 1 # … with 3,490 more rows Note that, for any patient and drug, value is 1 if the patient is taking the drug, and 0 otherwise. Therefore, for any particular patient, the sum of the value’s will be equal to the number of drugs the patient is taking. This can be computed with summarise() (using the sum() function) after grouping by patient: drugs_long %&gt;% group_by(pid) %&gt;% # define groups by pid summarise(number_of_drugs = sum(value)) %&gt;% # sumaries for each group ungroup() # undo the grouping # A tibble: 500 × 2 pid number_of_drugs &lt;dbl&gt; &lt;dbl&gt; 1 11 1 2 15 1 3 20 0 4 24 1 5 33 1 6 37 2 7 50 0 8 83 1 9 91 1 10 114 2 # … with 490 more rows In the previous example, we split the process in two steps to show the structure of the data after pivot_longer(), and the result of the mutate() statement used to recode the value 2 (corresponding to drugs not taken) to 0. However, the whole process can be done in a single step, as shown below: ah %&gt;% select(pid, bb:other) %&gt;% pivot_longer(bb:other, names_to = &quot;drug&quot;) %&gt;% # verticalize drugs mutate(value = ifelse(value == 2, 0, value)) %&gt;% # recode values: 2 -&gt; 0 group_by(pid) %&gt;% # define groups by pid summarise(number_of_drugs = sum(value)) %&gt;% # sumaries for each group ungroup() # undo the grouping # A tibble: 500 × 2 pid number_of_drugs &lt;dbl&gt; &lt;dbl&gt; 1 11 1 2 15 1 3 20 0 4 24 1 5 33 1 6 37 2 7 50 0 8 83 1 9 91 1 10 114 2 # … with 490 more rows 1.3.3 Example 2 As a second example, suppose we want to pick the last available BP measurement for each patient. Starting with the bp dataframe created previously, we can summarise measurements for sbp and dbp using the last() function, which picks the last value in each group (i.e., in the last row of each group): bp %&gt;% group_by(pid) %&gt;% summarise(last_sbp = last(sbp), # pick the last sbp for each patient last_dbp = last(dbp)) %&gt;% # pick the last dbp for each patient ungroup() # A tibble: 392 × 3 pid last_sbp last_dbp &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 140 80 2 15 130 90 3 20 120 80 4 24 140 90 5 33 140 75 6 37 146 70 7 83 150 90 8 91 150 100 9 114 125 80 10 145 150 85 # … with 382 more rows Note that the resulting dataframe contains a single row per patient, and the variables have the names provided in the summarise() function call. 1.4 Combining dataframes It is quite common to organise all the data collected in a clinical study in several dataframes, each having a relatively small number of thematically related variables. This is preferable to having a single dataframe packed with lots of variables. Moreover, different sets of variables may require a different tidy structure. For instance, demographic variables such as region, age and sex, are observed just once per patient, and therefore can be accomodated in a dataframe having one row per patient. However, blood presure values (sbp and dbp) may be observed at several visits, thus requiring several rows per patient (one for each visit) for an optimal, tidy structure. A better way to store the data in dataframe ah would be to split it in several dataframes, as suggested in table 1.6: Table 1.6: The DISEHTAE data distributed in four dataframes dataframe variables demo pid, data_xtract_dt, region, age, sex, ah_dx_dt bp pid, visit, sbp, dbp risk_factors pid, glucose, dx_dm, total_c, hdl_c, ldl_c, trigly, dx_dyslip, smoke, weight, height, dx_lvh, cv_risk_record, creatinine treatments pid, lmr, bb, diur, acei, arb, cbb, ab, other With the study data structured this way, the need of combining data from different dataframes will arise very soon. For instance, to compare the frequency of different treatments accoding to sex, dataframes demo and treatments should be combined. Of course, rows in both dataframes should be combined by patient, and this is why the pid variable should be present in all dataframes. 1.4.1 Joins The operation of combining dataframes on (one or more) common key variables (like pid) is called a join. Package dplyr includes several functions performing diferent type of joins, but all of them have a similar syntax: the first two arguments should be the dataframes to combine, and a further argument by = indicates the key variable(s) used to match rows. The functions differ in what they return when there are non-matching rows. The following figure illustrates the use of these functions and the result they produce: Table 1.7: Joins of dataframes a and b a x y 1 A 2 B 3 C b x z 1 10 2 20 4 40 left_join(a, b, by = “x”) x y z 1 A 10 2 B 20 3 C NA right_join(a, b, by = “x”) x y z 1 A 10 2 B 20 4 NA 40 inner_join(a, b, by = “x”) x y z 1 A 10 2 B 20 full_join(a, b, by = “x”) x y z 1 A 10 2 B 20 3 C NA 4 NA 40 In words: left_join(a, b, by =\"x\") returns all rows in a and matching rows in b. right_join(a, b, by =\"x\") returns all rows in b and matching rows in a. inner_join(a, b, by =\"x\") returns all matching rows. full_join(a, b, by =\"x\") returns all rows in a and b. For example, this will combine data from demo and treatments. Because both dataframes have all 500 patients, all four types of join will produce the same result. To verify it we use the dim() function, which returns the dimension of a dataframe, that is, its number of rows and columns: left_join(demo, treatments, by = &quot;pid&quot;) %&gt;% dim() [1] 500 14 right_join(demo, treatments, by = &quot;pid&quot;) %&gt;% dim() [1] 500 14 inner_join(demo, treatments, by = &quot;pid&quot;) %&gt;% dim() [1] 500 14 full_join(demo, treatments, by = &quot;pid&quot;) %&gt;% dim() [1] 500 14 All four joins above produced a dataframe with 500 rows and 14 variables. When the key variable has a different name in the two dataframes (e.g., x1 and x2), the names should be specified as a character vector in the by = argument (e.g., by = c(\"x1\", \"x2\")). When there is more than one matching variable, these should be specified as a vector. For example, suppose we had an additional dataframe devices specifying the BP measurement device used in every visit: sphygmomanometer or automatic BP measurement (ABPM). # A tibble: 1,519 × 3 pid visit device &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; 1 11 v1 ABPM 2 11 v2 ABPM 3 15 v1 sphygmomanometer 4 15 v2 sphygmomanometer 5 15 v3 sphygmomanometer 6 15 v4 ABPM 7 20 v1 sphygmomanometer 8 20 v2 ABPM 9 20 v3 sphygmomanometer 10 20 v4 ABPM # … with 1,509 more rows If we want to compare BP values among devices, we need to merge dataframes bp and devices by patient and visit, so that in this case we have two key variables. The following script shows how to: left_join(bp, devices, by = c(&quot;pid&quot;, &quot;visit&quot;)) # A tibble: 1,519 × 5 pid visit sbp dbp device &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 11 v1 130 80 ABPM 2 11 v2 140 80 ABPM 3 15 v1 130 80 sphygmomanometer 4 15 v2 120 80 sphygmomanometer 5 15 v3 120 90 sphygmomanometer 6 15 v4 130 90 ABPM 7 20 v1 130 85 sphygmomanometer 8 20 v2 120 80 ABPM 9 20 v3 140 80 sphygmomanometer 10 20 v4 130 80 ABPM # … with 1,509 more rows If the by = argument is omitted, variables having the same name in both dataframes will be used as keys for the matching. So, the following code produces the same result as before. Note the message issued informing on the key variables used to match rows. left_join(bp, devices) Joining, by = c(&quot;pid&quot;, &quot;visit&quot;) # A tibble: 1,519 × 5 pid visit sbp dbp device &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 11 v1 130 80 ABPM 2 11 v2 140 80 ABPM 3 15 v1 130 80 sphygmomanometer 4 15 v2 120 80 sphygmomanometer 5 15 v3 120 90 sphygmomanometer 6 15 v4 130 90 ABPM 7 20 v1 130 85 sphygmomanometer 8 20 v2 120 80 ABPM 9 20 v3 140 80 sphygmomanometer 10 20 v4 130 80 ABPM # … with 1,509 more rows This should be used with caution to avoid undesired results that may occur if either some of the intended key variables have different names in both dataframes, or an unintended key has the same name in both dataframes. When there is more than one matching row in one of the dataframes, variables from the other dataframe will be repeated. For instance, demo has one row per patient, and bp has several rows per patient. If joined, values of variables from demo will be repeated for all matching rows in bp: left_join(demo, bp) %&gt;% head() pid data_xtract_dt region age sex ah_dx_dt visit sbp dbp 1 11 2003-03-15 6 52 1 1998-01-01 v1 130 80 2 11 2003-03-15 6 52 1 1998-01-01 v2 140 80 3 15 2003-03-15 6 57 2 1997-01-01 v1 130 80 4 15 2003-03-15 6 57 2 1997-01-01 v2 120 80 5 15 2003-03-15 6 57 2 1997-01-01 v3 120 90 6 15 2003-03-15 6 57 2 1997-01-01 v4 130 90 A right-join is just the opposite of a left-join, so that left_join(a, b, by = \"x\") will produce the same result as right_join(b, a, by = \"x\"). The left_join of demo and bp will have all patients in demo. However, not all these patients have a matching row in bp, and therefore sbp and dbp will be missing for some rows: lj &lt;- left_join(demo, bp) nrow(lj) [1] 1627 colSums(is.na(lj)) pid data_xtract_dt region age sex 0 0 0 0 27 ah_dx_dt visit sbp dbp 759 108 108 108 We see that lj has 1627 rows, but 108 missings in both sbp and dbp. An inner join of demo and bp will not include these rows, since only matching rows are retained, but now there are no missings in sbp or dbp. ij &lt;- inner_join(demo, bp) nrow(ij) [1] 1519 colSums(is.na(ij)) pid data_xtract_dt region age sex 0 0 0 0 23 ah_dx_dt visit sbp dbp 683 0 0 0 Last, a full join of demo and bp will include patients appearing in either dataframe, even if no matching row is found in the other: ij &lt;- full_join(demo, bp) nrow(ij) [1] 1627 colSums(is.na(ij)) pid data_xtract_dt region age sex 0 0 0 0 27 ah_dx_dt visit sbp dbp 759 108 108 108 1.4.2 Example 1: left- and right-joins Suppose we want to exclude from analysis those patients with unknown (missing) sex or date of diagnosis of arterial hypertension. These are the only variables having missing values in demo: colSums(is.na(demo)) pid data_xtract_dt region age sex 0 0 0 0 9 ah_dx_dt 249 To get rid of cases with incomplete data, we can use function na.omit(), save the result as demo_complete, and see how many rows are left: demo_complete &lt;- na.omit(demo) # remove rows with missing values nrow(demo_complete) # how many rows? [1] 249 The dataframe demo_complete has 249 rows. Now suppose we want to analyze treatments by sex. We need to merge demo_complete with treatments, so that only patients in the former are retained. demo_treat &lt;- left_join(demo_complete, treatments, by = &quot;pid&quot;) head(demo_treat) pid data_xtract_dt region age sex ah_dx_dt lmr bb diur acei arb cbb ab 1 11 2003-03-15 6 52 1 1998-01-01 1 1 2 2 2 2 2 2 15 2003-03-15 6 57 2 1997-01-01 1 2 2 1 2 2 2 3 20 2003-05-13 1 62 1 2003-04-10 1 2 2 2 2 2 2 4 24 2003-05-27 10 69 2 1993-01-01 1 2 1 2 2 2 2 5 33 2003-05-28 10 70 2 1994-01-01 1 2 1 2 2 2 2 6 37 2003-06-14 9 64 2 1995-04-07 1 2 1 2 1 2 2 other 1 2 2 2 3 2 4 2 5 2 6 2 We see that the result includes all variables in both dataframes, but only rows in demo_complete (249 rows): nrow(demo_treat) [1] 249 Join functions can be chained just as any other function, and therefore the following code is equivalent to the previous one: demo_complete %&gt;% left_join(treatments, by = &quot;pid&quot;) %&gt;% head() pid data_xtract_dt region age sex ah_dx_dt lmr bb diur acei arb cbb ab 1 11 2003-03-15 6 52 1 1998-01-01 1 1 2 2 2 2 2 2 15 2003-03-15 6 57 2 1997-01-01 1 2 2 1 2 2 2 3 20 2003-05-13 1 62 1 2003-04-10 1 2 2 2 2 2 2 4 24 2003-05-27 10 69 2 1993-01-01 1 2 1 2 2 2 2 5 33 2003-05-28 10 70 2 1994-01-01 1 2 1 2 2 2 2 6 37 2003-06-14 9 64 2 1995-04-07 1 2 1 2 1 2 2 other 1 2 2 2 3 2 4 2 5 2 6 2 The advantage of chaining is that we can modify the first dataframe as needed, and then do the join. For instance, to keep the result as simple as possible, we may want to select only the needed variables in demo_complete before left-joining the resulting dataframe to treatments: demo_complete %&gt;% select(pid, sex) %&gt;% # to keep only needed variables left_join(treatments, by = &quot;pid&quot;) %&gt;% # left_join with treatments head() pid sex lmr bb diur acei arb cbb ab other 1 11 1 1 1 2 2 2 2 2 2 2 15 2 1 2 2 1 2 2 2 2 3 20 1 1 2 2 2 2 2 2 2 4 24 2 1 2 1 2 2 2 2 2 5 33 2 1 2 1 2 2 2 2 2 6 37 2 1 2 1 2 1 2 2 2 1.4.3 Example 2: inner- and full-joins Suppose we want to analyse some patient summaries of BP measurements, such as the number of visits in which BP was monitored, and the mean BP values of all measurements of a patient. First we need to compute these summaries for each patient: bp_summaries &lt;- bp %&gt;% group_by(pid) %&gt;% summarise(n_of_measurements = n(), mean_sbp = mean(sbp), mean_dbp = mean(dbp)) %&gt;% ungroup() bp_summaries # A tibble: 392 × 4 pid n_of_measurements mean_sbp mean_dbp &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 2 135 80 2 15 4 125 85 3 20 5 128 81 4 24 6 146. 88.3 5 33 6 139. 80.8 6 37 6 152. 76.8 7 83 2 148. 87.5 8 91 1 150 100 9 114 2 132. 75 10 145 2 145 82.5 # … with 382 more rows Now we should merge demo_complete and bp_summaries, but they have a different number of rows (249 and 392, respectively): some patients are present in both dataframes, but some others are present in just one of them. An inner join will keep only patients that are present in both dataframes. Because neither demo_complete nor bp_summaries have missings, the resulting dataframe is free of missings: inner &lt;- inner_join(demo_complete, bp_summaries) nrow(inner) [1] 217 colSums(is.na(inner)) pid data_xtract_dt region age 0 0 0 0 sex ah_dx_dt n_of_measurements mean_sbp 0 0 0 0 mean_dbp 0 However, a full join will include patients appearing in any dataframe, and therefore, patients not appearing in one of them will have missings in variables comming from the other: full &lt;- full_join(demo_complete, bp_summaries) nrow(full) [1] 424 colSums(is.na(full)) pid data_xtract_dt region age 0 175 175 175 sex ah_dx_dt n_of_measurements mean_sbp 175 175 32 32 mean_dbp 32 1.4.4 Binding Sometimes the data of a study is received in different batches, each batch including different patients. For instance, suppose the ah data was received in three data files containing different patients. Each of these files would be read, producing one dataframe each. To simulate this situation, the following code splits the ah dataframe in three parts, using the slice() function: batch_1 &lt;- slice(ah, 1:200) batch_2 &lt;- slice(ah, 201:400) batch_3 &lt;- slice(ah, 401:500) In such a case, the rows in each dataframe should be binded into a single dataframe. This can be done with function bind_rows(): batches_123 &lt;- bind_rows(batch_1, batch_2, batch_3) nrow(batches_123) [1] 500 The resulting dataframe batches_123 has now all 500 rows. If some of the batches contained a variable not present in the remaining dataframes, this variable would be missing in rows coming from the later. An analogous function bind_cols() exists, allowing to bind the columns of several dataframes. However, the binding is done by row position, not by value of a common key variable, which limits its usefulness. Resources Data transformation with dplyr and Data wrangling are two cheatsheets, very handy for a fast look-up of the main functions in packages dplyr and tidyr. Worried about understanding joins? See these annimations! A complete catalog of argument variations in dplyr::select(). A collection of lesser known but useful dplyrfunctions is presented here. A tidyr tutorial where you can learn about other useful functions in this package. For complex recoding of variables, this is a good explanation on how to use the case_when() function in dplyr, with examples. A comparison of dplyr funtions to their base R equivalents can be found here. If you work with really HUGE datasets you may want to know about the data.table package for more efficient dataframe operations. A comparison to of data.table and dplyr can be found here. Exercises Read the DISEHTAE data and create the four dataframes defined in table 1.6. Starting from dataframe bp, create a new dataframe bp_monitoring, with one row per patient and two variables: pid and num_bpm, the last one being the number of blood pressure measurements (i.e., the number of visits in which BP was measured). Starting from dataframe treatments, create a new dataframe num_drugs with one row per patient and two variables: pid and num_drugs, the last one being the number of drugs a patient is taking. Compute the number of cardiovascular risk factors for each patient (overweight defined as BMI of 25 kg/m^2 or more, diabtes mellitus, dyslipidemia, left venticular hypertrophy, current smoker), assuming that when a risk factor is missing, it is not present. Proceed as follows, starting from dataframe ah: select pid and relevant variables. use mutate() to create new variables overweight and current_smoker, as 1 (for yes) or 2 (for no). select pid and yes/no variables (coded as 1/2). use across() in a mutate() statement to recode all yes/no variables so that they take values 1 (for yes) or 0 (for no). reshape to long format group the dataframe by patient and compute the sum of values for each patient. ungroup the dataframe. You should end up with a dataframe num_risk_factors having only two variables: pid and num_cvrf (number of cardiovascular risk factors). What is the distribution of num_cvrf? How many patients have none, one, two, three, four, or five risk factors? Using the %&gt;% operator to chain operations, do the following: Starting with demo, combine it with num_drugs, so that only patients with complete demographic data appear in the result. Then combine the previous result with num_cvrf, so that only patients with complete demographic data appear in the result. Then combine the previous result with bp_monitoring, so that only patients with complete demographic data appear in the result. You should end up with a dataframe having the following variables: pid, age, sex, num_drugs, num_cvrf and num_bpm. The package gapminder includes a dataframe of the same name containing data on life expectancy by year and country of the world: library(gapminder) gapminder # A tibble: 1,704 × 6 country continent year lifeExp pop gdpPercap &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 2 Afghanistan Asia 1957 30.3 9240934 821. 3 Afghanistan Asia 1962 32.0 10267083 853. 4 Afghanistan Asia 1967 34.0 11537966 836. 5 Afghanistan Asia 1972 36.1 13079460 740. 6 Afghanistan Asia 1977 38.4 14880372 786. 7 Afghanistan Asia 1982 39.9 12881816 978. 8 Afghanistan Asia 1987 40.8 13867957 852. 9 Afghanistan Asia 1992 41.7 16317921 649. 10 Afghanistan Asia 1997 41.8 22227415 635. # … with 1,694 more rows Using this dataframe, answer the following questions: What was the population of Europe in 2007? What was the average life expectancy for European countries in 2007? and what were minimum and maximum life expectancy values in Europe? Produce a graphic to see the evolution of the average life expectancy by continent from 1960 to 2000. "],["working-with-variables.html", "2 Working with variables 2.1 Strings 2.2 Factors 2.3 Dates 2.4 Dataframe variables Resources Exercises", " 2 Working with variables In virtually all studies, the data preparation process involves computing new variables or modifying some of the existing ones. The operations needed to modify an existing variable, or to create a new one, are typically dependent on the type of the input variable(s). For instance, numerical variables (such as the body weight and height) may be used to compute another numeric variable (the body mass index) with some formula; factors (such as the study site in multicentric studies) are often used to create a new variable (such as region or country) using some classification algorithm; and character variables (like symptoms) may require homogenization tasks (such as capitalization or elimination of whitespaces). It is therefore important to know what are the tools available to work with different types of variables, and how to use them. In this chapter, we will see how to perform the most common operations on character vectors (or strings), factors and dates. 2.1 Strings Strings of characters are almost always found among the data collected in a clinical study. The description of symptoms, signs, diseases, adverse events and treatments are examples of textual data. These kind of data is often encoded using controlled vocabularies, like ICD-9 or ICD-10 for diseases, MedDRA for adverse events, or ATC for drugs. However, proper encoding using these vocabularies is a non-trivial task that requires a very good knowledge of the coding vocabulary, and it is not always affordable or cost-effective, particularly in small studies. In addition, there is not always a controlled vocabulary available for the textual data at hand. For these reasons, working with strings is a common need in clinical and other type of studies. Although base R has functions to work with character vectors, we will give preference to those in package stringr, which is part of teh tidyverse. So, let’s start by loading this package: library(tidyverse) 2.1.1 Whitespaces and capitalization When working with real data, we will often find character vectors having semantically equivalent, but nonetheless different elements, due to whitespaces and capitalization. Consider the following character vector: x &lt;- c(&quot;Arterial hypertension&quot;, &quot;arterial hypertension&quot;, &quot;ARTERIAL HYPERTENSION&quot;, &quot;arterial hypertension &quot;, &quot;arterial hypertension &quot;, &quot;Pneumonia&quot;, &quot;pneumonia&quot;, &quot; pneumonia&quot;) x [1] &quot;Arterial hypertension&quot; &quot;arterial hypertension&quot; [3] &quot;ARTERIAL HYPERTENSION&quot; &quot;arterial hypertension &quot; [5] &quot;arterial hypertension &quot; &quot;Pneumonia&quot; [7] &quot;pneumonia&quot; &quot; pneumonia&quot; Although only two conditions appear in x (arterial hypertension and pneumonia), a frequency table of this vector will be less than satisfying, because all elements are actually different: some are lowercase, others contain uppercase letters, and some contain extra whitespaces (at the beginning, at the end, or between words): data.frame(x) %&gt;% count(x) x n 1 pneumonia 1 2 arterial hypertension 1 3 arterial hypertension 1 4 Arterial hypertension 1 5 ARTERIAL HYPERTENSION 1 6 arterial hypertension 1 7 pneumonia 1 8 Pneumonia 1 Because R is case sensitive, \"Arterial hypertension\" is not the same as \"arterial hypertension\". In addition, it is important to be aware that a white space (or blank) is a character in its own right, and therefore these two strings are different because of a trailing blank in the second one: &quot;arterial hypertension&quot; == &quot;arterial hypertension &quot; [1] FALSE Function str_trim() removes white spaces at the beginning or at the end of a string: str_trim(x) [1] &quot;Arterial hypertension&quot; &quot;arterial hypertension&quot; [3] &quot;ARTERIAL HYPERTENSION&quot; &quot;arterial hypertension&quot; [5] &quot;arterial hypertension&quot; &quot;Pneumonia&quot; [7] &quot;pneumonia&quot; &quot;pneumonia&quot; In the result above, elements two and four of x are still different, because the later has more than one white space between the two words. Function str_squish() not only removes leading and trailing white spaces, but also reduces repeated white spaces inside a string: str_squish(x) [1] &quot;Arterial hypertension&quot; &quot;arterial hypertension&quot; &quot;ARTERIAL HYPERTENSION&quot; [4] &quot;arterial hypertension&quot; &quot;arterial hypertension&quot; &quot;Pneumonia&quot; [7] &quot;pneumonia&quot; &quot;pneumonia&quot; After getting rid of white spaces, we can use any of the following functions to homogenize case: str_to_lower(), str_to_upper(), str_to_title() or str_to_sentence(). Here we use the last one (but you can try with the other three): x %&gt;% str_squish() %&gt;% # removes leading, trailing and repeated blanks str_to_sentence() # capitalizes the first letter [1] &quot;Arterial hypertension&quot; &quot;Arterial hypertension&quot; &quot;Arterial hypertension&quot; [4] &quot;Arterial hypertension&quot; &quot;Arterial hypertension&quot; &quot;Pneumonia&quot; [7] &quot;Pneumonia&quot; &quot;Pneumonia&quot; The two conditions appearing in x have now a homogeneous spelling, so that a decent frequency table can be produced, as shown below. In the scripts above we started working with an isolated character vector (x) . But what if this is a variable in a dataframe? The same functions can be used (and chained!) within a mutate statement to compute a new version of x: d &lt;- data.frame(x) d x 1 Arterial hypertension 2 arterial hypertension 3 ARTERIAL HYPERTENSION 4 arterial hypertension 5 arterial hypertension 6 Pneumonia 7 pneumonia 8 pneumonia d %&gt;% mutate(x = x %&gt;% str_squish() %&gt;% str_to_sentence()) %&gt;% count(x) # frequency table of x values x n 1 Arterial hypertension 5 2 Pneumonia 3 Now the frequency table looks as it should. 2.1.2 Matching, locating and replacing patterns Sometimes we will need to match some defined pattern in a character vector, with any of these purposes: Detect which elements in the vector match the pattern. Locate the position at which the pattern occurs in each element of the vector. Replace the pattern with an alternative text. Count how many times a pattern appears in each element of the vector. Functions str_detect(), str_locate(), str_replace(), and str_count() serve these four purposes, respectively. In all three functions, a pattern argument has to be specified. Consider the following vector describing the medical antecedents of 3 patients: x &lt;- c(&quot;diabetes mellitus&quot;, &quot;asthma&quot;, &quot;bronchial asthma + DM.&quot;) x [1] &quot;diabetes mellitus&quot; &quot;asthma&quot; &quot;bronchial asthma + DM.&quot; Suppose we want to identify patients with bronchial asthma. Patients 2 and 3 have different descriptors, but both contain “asthma”. Function str_detect() will tell us which elements in x contain “asthma”, by returning a logical vector the same length of x, with TRUE when the pattern is detected or FALSE otherwise: str_detect(x, pattern = &quot;asthma&quot;) [1] FALSE TRUE TRUE In some cases we may need to know what are the positions where a pattern is found. This can be achieved with function str_locate(). Note the result is a matrix with as many rows as elements in x, and two columns indicating the start-ing and end-ing positions of the pattern (or NA for non-matching elements in x). str_locate(x, &quot;asthma&quot;) start end [1,] NA NA [2,] 1 6 [3,] 11 16 Now suppose we want to replace “DM” by the full description “diabetes mellitus”. This can be done with str_replace(), specifying the pattern to be replaced and the replacement text: str_replace(x, pattern = &quot;DM&quot;, replacement = &quot;diabetes mellitus&quot;) [1] &quot;diabetes mellitus&quot; [2] &quot;asthma&quot; [3] &quot;bronchial asthma + diabetes mellitus.&quot; Last, str_count()counts the number of appearances of the pattern in each element of the input vector: x &lt;- c(&quot;I&quot;, &quot;II&quot;, &quot;III.&quot;, &quot;IV&quot;) str_count(x, pattern = &quot;I&quot;) [1] 1 2 3 1 The previous examples worked because the specified patterns contain letters only. However, when the pattern contains special characters, such as the dot, this will produce an error or unexpected results, as in this case: str_detect(x, pattern = &quot;.&quot;) [1] TRUE TRUE TRUE TRUE The reason for the previous result is that, by default, str_detect() expects the pattern to be specified as a regular expression. Regular expression are introduced in the next section, but for the time being, a turnaround is to use the helper function fixed() to specify the pattern literally (or coll() for languages other than English, see ?fixed). Now the result is as expected: str_detect(x, pattern = fixed(&quot;.&quot;)) [1] FALSE FALSE TRUE FALSE 2.1.3 Regular expressions Regular expressions (shortened as regex or regexp) provide a very flexible and concise way to specify complex patterns in strings through the use of symbols with special meaning called meta characters. These are listed in table 2.1 and examples of use follow. Table 2.1: Meta characters used in regular expressions Meta character Meaning . Matches any single character, except a line break | Matches the previous or the next characters ^ Denotes the begining of the string $ Denotes the end of the string [ ] Matches any character contained between them [^ ] Matches any character NOT contained between them ( ) Matches the characters contained between them in exact order ? Makes the preceeding optional * Matches 0 or more repetitions of the preceeding + Matches 1 or more repetitions of the preceeding {n} Matches exactly n repetitions of the preceeding {n,m} Matches a minimum of “n” and a maximum of “m” repetitions of the preceding \\ Escapes the next character The period . is used as a wildcard. Here we use it to match not only \"diabetes\", but also the misspelled \"diabetis\": x &lt;- c(&quot;diabetes mellitus&quot;, &quot;diabetis&quot;, &quot;DM&quot;, &quot;non-diabetic&quot;, &quot;carotid stenosis&quot;) str_detect(x, pattern = &quot;diabet.s&quot;) [1] TRUE TRUE FALSE FALSE FALSE The vertical bar | is used to specify alternative patterns (read it as or) to be matched. Here we use it to match “diabet.s” or the abbreviation for diabetes mellitus, “DM”: str_detect(x, pattern = &quot;diabet.s|DM&quot;) [1] TRUE TRUE TRUE FALSE FALSE The caret ^ and the dollar $ are used as anchors, to indicate the begining and the end of the string, respectively. Here we use them to identify drug families (cephalosporins and monoclonal antibodies): x &lt;- c(&quot;cefaclor&quot;, &quot;cefixime&quot;, &quot;diphenhydramine acefyllinate&quot;) str_detect(x, pattern = &quot;^cef&quot;) [1] TRUE TRUE FALSE x &lt;- c(&quot;omalizumab&quot;, &quot;reslizumab&quot;, &quot;mabuterol&quot;) str_detect(x, pattern = &quot;mab$&quot;) [1] TRUE TRUE FALSE Square brackets [] are used to specify a class of characters, so that any character in the class will be matched. Classes can be specified by a sequence of characters, as in [abc], [ABC] or [123], or using the dash to indicate a range, as in [a-z], [A-Z], or [0-9]. In the following example we use function str_extract() to get the doses and units from character vector x. The + is a quantifier, indicating to pick the pevious one or more times (try without the + to see the difference): x &lt;- c(&quot;250 mg&quot;, &quot;500mg&quot;, &quot;1 g&quot;, &quot;1gram&quot;, &quot;250 MG&quot;) str_extract(x, pattern = &quot;[0-9]+&quot;) # any number [1] &quot;250&quot; &quot;500&quot; &quot;1&quot; &quot;1&quot; &quot;250&quot; str_extract(x, pattern = &quot;[a-zA-Z]+&quot;) # any alphabetic character [1] &quot;mg&quot; &quot;mg&quot; &quot;g&quot; &quot;gram&quot; &quot;MG&quot;   Some classes of characters are so common that they are given special names: str_extract(x, pattern = &quot;[:digit:]+&quot;) # same as &quot;[0-9]+&quot; [1] &quot;250&quot; &quot;500&quot; &quot;1&quot; &quot;1&quot; &quot;250&quot; str_extract(x, pattern = &quot;[:alpha:]+&quot;) # same as &quot;[a-zA-Z]+&quot; [1] &quot;mg&quot; &quot;mg&quot; &quot;g&quot; &quot;gram&quot; &quot;MG&quot; When the caret ^ is used within [], it means negation, that is, to match any character not in the specified class. For instance, in the following script, we first extract roman numerals I to IV from vector nyha, and then we extract anything which is not one of them: nyha &lt;- c(&quot;I. No limitation in normal physical activity&quot;, &quot;II. Mild symptoms only in normal activity&quot;, &quot;III. Marked symptoms during daily activities, asymptomatic only at rest&quot;, &quot;IV. Severe limitations, symptoms even at rest&quot;) str_extract(nyha, pattern = &quot;[IV]+&quot;) # any I or V [1] &quot;I&quot; &quot;II&quot; &quot;III&quot; &quot;IV&quot; str_extract(nyha, pattern = &quot;[^IV]+&quot;) # anything different from I and V [1] &quot;. No limitation in normal physical activity&quot; [2] &quot;. Mild symptoms only in normal activity&quot; [3] &quot;. Marked symptoms during daily activities, asymptomatic only at rest&quot; [4] &quot;. Severe limitations, symptoms even at rest&quot; Brackets () are used to define a group of characters in the specified sequence. For instance, suppose we want to extract the units from vector x. Note the difference with square brackets: x [1] &quot;250 mg&quot; &quot;500mg&quot; &quot;1 g&quot; &quot;1gram&quot; &quot;250 MG&quot; str_detect(x, pattern = &quot;[mg]&quot;) # match any m or g, lowercase [1] TRUE TRUE TRUE TRUE FALSE str_detect(x, pattern = &quot;(mg)&quot;) # match the sequence &quot;mg&quot; [1] TRUE TRUE FALSE FALSE FALSE The question mark ? is used to make the preceeding optional. Here we use it to match two alternative spellings of some words: x &lt;- c(&quot;haemorrhage&quot;, &quot;hemorrhage&quot;, &quot;hemoglobin decreased&quot;, &quot;haemoglobin decreased&quot;) str_detect(x, pattern = &quot;ha?emo&quot;) [1] TRUE TRUE TRUE TRUE Braces {} are used to define number of repetitions, either exact, or in an interval: x &lt;- c(&quot;AMI in 2019&quot;, &quot;Born in 1989&quot;, &quot;Pulse 75 bpm&quot;) str_extract(x, pattern=&quot;[0-9]{4}&quot;) # exactly four digits [1] &quot;2019&quot; &quot;1989&quot; NA x &lt;- c(&quot;500 mg/24h&quot;, &quot;1000 mg/24h&quot;, &quot;week 1: 750 mg daily&quot;) str_extract(x, pattern=&quot;[0-9]{3,4}&quot;) # between three and four digits [1] &quot;500&quot; &quot;1000&quot; &quot;750&quot; The asterisk * allows to match any number of repetitions of an optional part: x &lt;- c(&quot;ab&quot;, &quot;aab&quot;, &quot;aaaab&quot;, &quot;b&quot;, &quot;c&quot;) str_detect(x, pattern=&quot;a*b&quot;) [1] TRUE TRUE TRUE TRUE FALSE Last, the backslash (\\) allows to remove the special meaning of meta characters, when we need to match them literally. However, because the backslash is a meta character itself, we need to remove its special meaning with another backslash, so that wee need to write two of them: x &lt;- c(&quot;asthma&quot;, &quot;astha + DM&quot;, &quot;AHT + hyperlipidemia&quot;) str_detect(x, &quot;\\\\+&quot;) [1] FALSE TRUE TRUE Working with regular expressions is challenging, particularly as they increase in complexity, and a useful trick is to visualize the matches they produce. A couple of functions help with this: str_view() shows the first match, and str_view_all() shows all matches: x &lt;- c(&quot;haemorrhage&quot;, &quot;hemorrhage and hemoglobin decreased&quot;, &quot;haemoglobin decreased&quot;) str_view(x, pattern = &quot;ha?emo&quot;) str_view_all(x, pattern = &quot;ha?emo&quot;) 2.1.4 Subsetting strings When we need to extract a specific part of a string defined by its position, function str_sub() is the way to go. Consider for instance the CIP codes, the unique personal identifier used by the Catalan Healthcare System, composed by 14 characters with the following structure: “iiiisyymmddxxx”, where “iiii” are surname initials, “s” is a code for sex (0: men, 1:female), “yymmdd” is the birth date, and “xxx” are three random numbers. Here we extract different parts of the CIP code based on start and end positions: x &lt;- c(&quot;FEGA0720525000&quot;, &quot;ROMA1680913000&quot;, &quot;PEJI0590503&quot;) str_sub(x, start = 5, end = 5) # extracts sex code [1] &quot;0&quot; &quot;1&quot; &quot;0&quot; str_sub(x, start = 6, end = 7) # extracts year of birth [1] &quot;72&quot; &quot;68&quot; &quot;59&quot; str_sub(x, start = 6, end = 11) # extracts birthdate [1] &quot;720525&quot; &quot;680913&quot; &quot;590503&quot; 2.1.5 Other useful functions Function str_length() finds the number of characters for each element of the input vector: x &lt;- c(&quot;DM&quot;, &quot;AHT&quot;) str_length(x) [1] 2 3 Sometimes we may want to collapse all the elements of a character vector into a single string (a character vector of length one). This can be done with str_flatten(): str_flatten(x, collapse = &quot; and &quot;) [1] &quot;DM and AHT&quot; With str_glue() we can easily compose texts, inserting values as needed, which may be useful to produce narrative reports. Note the use of str_flatten() to define antecedents: pid &lt;- 10 age &lt;- 58 sex &lt;- &quot;male&quot; antecedents &lt;- str_flatten(x, collapse = &quot; and &quot;) adverse_event &lt;- &quot;cephalea&quot; study_day &lt;- 3 str_glue(&quot;Patient number {pid}, a {age}y {sex} with {antecedents}, reported {adverse_event} on study day {study_day}.&quot;) Patient number 10, a 58y male with DM and AHT, reported cephalea on study day 3. Function str_split() splits up a string into pieces by defining a pattern used as separator, and returns a list the same length of the input vector. In the following example we split antecedents of a patient separated by a +. We need to use the (double) backslash in pattern to refer to literal +, and not to meta character +: x &lt;- c(&quot;asthma&quot;, &quot;astha + DM&quot;, &quot;DM + AHT + hyperlipidemia&quot;) str_split(x, pattern = &quot;\\\\+&quot;) [[1]] [1] &quot;asthma&quot; [[2]] [1] &quot;astha &quot; &quot; DM&quot; [[3]] [1] &quot;DM &quot; &quot; AHT &quot; &quot; hyperlipidemia&quot; The result is a list of character vectors, each having as many elements as antecedents. We can use the option simplify = TRUE to get the result as a matrix instead, with as many rows as elements in the input vector, and as many columns as the maximum number of antecedents, three in this example. In this case, some entries will be empty for patients with less than three antecedents: str_split(x, pattern = &quot;\\\\+&quot;, simplify = TRUE) [,1] [,2] [,3] [1,] &quot;asthma&quot; &quot;&quot; &quot;&quot; [2,] &quot;astha &quot; &quot; DM&quot; &quot;&quot; [3,] &quot;DM &quot; &quot; AHT &quot; &quot; hyperlipidemia&quot; Function str_sort() will sort the elements of a character vector alphabetically, unless we use option numeric=TRUE: x &lt;- c(&quot;10 mg&quot;, &quot;5 mg&quot;, &quot;2.5 mg&quot;) str_sort(x, numeric = TRUE) [1] &quot;2.5 mg&quot; &quot;5 mg&quot; &quot;10 mg&quot; 2.2 Factors Package forcats, which is part of the tidyverse, has functions to address common needs when working with factors. To illustrate some of these functions we will use the DISETHAE data introduced in the previous chapter. Here we read the data and define a factor for the categorical variable region: library(readxl) ah &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% mutate(region = factor(region, levels = 1:17, labels = c(&quot;Andalucía&quot;, &quot;Aragón&quot;, &quot;Asturias&quot;, &quot;Baleares&quot;, &quot;Canarias&quot;, &quot;Cantabria&quot;, &quot;Castilla-La Mancha&quot;, &quot;Castilla-León&quot;, &quot;Catalunya&quot;, &quot;Extremadura&quot;, &quot;Galicia&quot;, &quot;La Rioja&quot;, &quot;Madrid&quot;, &quot;Murcia&quot;, &quot;Navarra&quot;, &quot;País Vasco&quot;, &quot;Valencia&quot;))) 2.2.1 Reordering levels Suppose we want to produce a bar chart to see the number of patients recruited in each region. Here we use function gf_barh() from ggformula to produce a horizontal barchart: library(ggformula) gf_barh(~region, data = ah) We could make the bar chart more readable if regions were sorted by frequency. This is precisely the purpose of fct_infreq(): gf_barh(~fct_infreq(region), data = ah, ylab = &quot;Regions&quot;) To sort them in descending frequency we can use fct_rev(), which reverses the order of the levels of a factor. Note how we pipe this function after fct_infreq(region): gf_barh(~fct_infreq(region) %&gt;% fct_rev(), data = ah, ylab = &quot;Regions&quot;) Sometimes we may want to order the levels of a factor by some other variable. For instance, suppose we want to explore the mean age of patients in each region. To this end, we first group by region to compute the mean for each region, ungoup, and finally produce a graphic showing mean ages by region: ah %&gt;% group_by(region) %&gt;% summarize(mean_age = mean(age)) %&gt;% ungroup() %&gt;% gf_point(region ~ mean_age) Again, this would be much more readable with regions sorted by the mean age. This can be done by passing variable mean_age as second argument to function fct_reorder(): ah %&gt;% group_by(region) %&gt;% summarize(mean_age = mean(age)) %&gt;% ungroup() %&gt;% gf_point(fct_reorder(region, mean_age) ~ mean_age, ylab = &quot;Regions&quot;) 2.2.2 Modifying levels Changing the levels of a factor may be motivated by several reasons, such as providing better names, or grouping some levels into broader categories. There are several functions available in forcats to change the levels of a factor, some of which are very specialized. The most flexible one is fct_recode(). Here we use it to rename some of the regions according to official names. Note that new names are equated to old names (in this order), and that levels not mentioned will remain unchanged: ah %&gt;% mutate(region = fct_recode(region, &quot;Principado de Asturias&quot; = &quot;Asturias&quot;, &quot;Comunidad Foral de Navarra&quot; = &quot;Navarra&quot;, &quot;Comunitat Valenciana&quot; = &quot;Valencia&quot;, &quot;Castilla y León&quot; = &quot;Castilla-León&quot;, &quot;Cominudad de Madrid&quot; = &quot;Madrid&quot;)) %&gt;% gf_barh(~fct_infreq(region) %&gt;% fct_rev(), ylab = &quot;Regions&quot;) If we need to group levels into broader categories, str_collapse() is useful, as shown in the script below. In the second argument to this function, each new (broader) level is equated to a vector of (old) levels. Again, note that levels not mentioned are kept unchanged (“Canarias” still appears in the plot since it is not classified in any of the new broader regions). ah %&gt;% mutate(region = fct_collapse(region, &quot;North&quot; = c(&quot;Galicia&quot;, &quot;Asturias&quot;, &quot;Cantabria&quot;, &quot;País Vasco&quot;, &quot;Navarra&quot;, &quot;La Rioja&quot;, &quot;Aragón&quot;), &quot;East&quot; = c(&quot;Catalunya&quot;, &quot;Valencia&quot;, &quot;Murcia&quot;), &quot;South&quot; = &quot;Andalucía&quot;, &quot;Middle-West&quot; = c(&quot;Castilla-La Mancha&quot;, &quot;Castilla-León&quot;, &quot;Madrid&quot;, &quot;Extremadura&quot;))) %&gt;% gf_barh(~fct_infreq(region) %&gt;% fct_rev(), ylab = &quot;Regions&quot;) Sometimes we may need to collapse the levels of a factor which are less represented in the data. Suppose this is the distribution of the number of patients recruited in a clinical trial: d &lt;- data.frame(center = factor(paste(&quot;Hospital&quot;,rep(LETTERS[1:8], times = c(51, 26, 4, 5, 6, 32, 16, 6))))) d %&gt;% gf_barh(~center %&gt;% fct_rev(), ylab = &quot;&quot;) In such a case, we may want to pool small centers before analysis to avoid a large imbalance of centers. If none of its optional arguments is used, fct_lump() will pool smaller centers ensuring that the frequency of the pool will be lower than the remaining (unpooled) centers: d %&gt;% gf_barh(~fct_lump(center) %&gt;% fct_infreq() %&gt;% fct_rev(), ylab = &quot;&quot;) Not very useful in this case, since the Other class only contains hospital C. However, we can controll how the pooling is done using one of the two optional arguments n or prop. With n we can state how many centers should be kept unpooled. With prop we can indicate that centers with a frequency lower than prop should be pooled: d %&gt;% gf_barh(~fct_lump(center, n = 4) %&gt;% fct_infreq() %&gt;% fct_rev(), ylab = &quot;&quot;) d %&gt;% gf_percentsh(~fct_lump(center, prop = .15) %&gt;% fct_infreq() %&gt;% fct_rev(), ylab = &quot;&quot;) By default, the pool is named “Other”, but the optional argument other_level allows to specify a different name: d %&gt;% gf_barh(~fct_lump(center, n = 4, other_level = &quot;Small centers&quot;) %&gt;% fct_rev(), ylab = &quot;&quot;) 2.2.3 Ordered factors A special type of factors are ordered factors. The concept of an ordered factor may be a bit confusing, because the levels of a (regular) factor are always defined in a certain order, which affects how they are displayed in outputs like frequency tables or graphics. So, what is the point of ordered factors? In fact, the most important reason is their use in statistical models, which is different for (regular) factors and ordered factors. Ordered factors can be defined with function ordered(), a variation of factor(). Consider the following example, where we create both a (regular) factor and an ordered factor from a character vector of NYHA classes. You will note that, when printed to the console, the levels of nyha_1 and nyha_2 are displayed in a slightly different way (Levels: I II III IV and Levels: I &lt; II &lt; III &lt; IV, respectively): nyha &lt;- sample(c(&quot;I&quot;, &quot;II&quot;, &quot;III&quot;, &quot;IV&quot;), 10, replace = TRUE) nyha_1 &lt;- factor(nyha) # defines a (regular) factor nyha_2 &lt;- ordered(nyha) # defines an ordered factor nyha_1 [1] IV IV IV IV III II III I III IV Levels: I II III IV nyha_2 [1] IV IV IV IV III II III I III IV Levels: I &lt; II &lt; III &lt; IV   In addition, the class of nyha_2 is not just \"factor\", but \"ordered\" as well: class(nyha_1) [1] &quot;factor&quot; class(nyha_2) [1] &quot;ordered&quot; &quot;factor&quot; The behavior of both regular factors and ordered factors is the same in most cases. For instance, if we produce frequency tables or graphics, the result will be exactly the same: library(patchwork) ggformula::gf_bar(~nyha_1) + ggformula::gf_bar(~nyha_2) As commented above, the most important case where a regular or an ordered factor makes a difference is statistical modeling. In any other instance, it is unlikely that you need to define an ordered factor. Although we will not discuss the use of ordered factors in statistical models here, we want to make you aware of their existence, just in case you come across them. 2.3 Dates R has several data structures (object classes) and functions to work with dates and date-time values, providing the functionality one may possibly need, even to deal with data from different timezones. However, the lubridate package makes it easier to tackle many of the common tasks needed when processing date or date-time data in clinical studies. Because lubridate is not part of the tidyverse, we need to start by loading this package: library(lubridate) 2.3.1 Data structures for date and date-time values Let’s start by looking at the difference between dates and dete-time values. While dates identify calendar days, date-time values identify a point in time (to the nearest second). A couple of functions in lubridate are useful to see the difference between dates and date-time values: today(), which returns today’s date, and now(), which returns the current date-time: today() # today&#39;s date, a date value [1] &quot;2023-01-18&quot; now() # current time, a date-time value [1] &quot;2023-01-18 11:02:01 CET&quot; Note that the date-time value produced by now() specifies a timezone: CET (standing for Central European Time). By default, this will be your computer’s system timezone, but we can specify any timezone with argument tz, including UTC (corresponding to Coordinated Universal Time: now(tz = &quot;WET&quot;) # Western European Time (e.g., Canary Islands) [1] &quot;2023-01-18 10:02:01 WET&quot; now(tz = &quot;Pacific/Auckland&quot;) # City of New Zealand [1] &quot;2023-01-18 23:02:01 NZDT&quot; now(tz = &quot;UTC&quot;) # Coordinated Universal Time [1] &quot;2023-01-18 10:02:01 UTC&quot; Using class() we can see what are the types of objects used to store dates and date-times in R: x &lt;- today() y &lt;- now() class(x) [1] &quot;Date&quot; class(y) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; As you see, dates are stored in Date objects, while date-times are stored as POSIXct (or POSIXt) objects. As we saw in previous sections, date-time values can be converted to date objects with as.Date(): as.Date(y) # date-time to date [1] &quot;2023-01-18&quot; class(as.Date(y)) [1] &quot;Date&quot; 2.3.2 Date and date-time formats By default, R uses the ISO 8601 standard notation to specify dates and date-times. However, alternative notations can be used by means of conversion specifications. The following table shows the most common symbols used in conversion specifications (for a more complete list, see ?strptime): Table 2.2: Symbols commonly used in Date and time conversion specifications Symbol Meaning %a abbreviated weekday name %A full weekday name %d day of the month %b abbreviated month name %B full month name %m month as number %y year without century %Y year with century %H hour %M minute %S second %T equivalent to %H:%M:%S These conversion specifications are used in a format string (e.g., \"%d %B %Y\"). Any other character included in the format string is interpreted literally. Function format() can be used to create a character string with the desired notation, as shown in these examples: format(x, format = &quot;%A %d %B %Y&quot;) [1] &quot;Wednesday 18 January 2023&quot; format(y, &quot;%d-%b-%Y, %T %Z&quot;) [1] &quot;18-Jan-2023, 11:02:01 CET&quot; Similarly, we can read dates from character vectors expressing calendar dates in an specific notation, by specifying it in the format argument of as.Date(): as.Date(&quot;05/03/2001&quot;, format = &quot;%d/%m/%Y&quot;) [1] &quot;2001-03-05&quot; 2.3.3 Reading dates Probably the most common problem with dates arises when reading data from external files in which calendar dates are indicated in strings using a specific notation (like “dd/mm/YYYY”). As we have seen in the previous section, these can be read and converted to date objects with as.Date(), provided the notation is consistent. However, inconsistent notation of dates is not uncommon, and cannot be handled by as.Date(). Fortunately, lubridate includes a family of functions that are able to parse dates from input vectors containing mixed date notations, provided the ordering of the date components is always the same. These functions are named according to the order of the date components. For instance, the following strings contain highly heterogeneous date notations, but the components are always in the same order (day, month and year), so that we can use function dmy() to read them: x &lt;- c(&quot;15-sep-03&quot;, &quot;24/08/1987&quot;, &quot;Tuesday 25 March 1958&quot;, &quot;02121957&quot;) dmy(x) [1] &quot;2003-09-15&quot; &quot;1987-08-24&quot; &quot;1958-03-25&quot; &quot;1957-12-02&quot; Similar functions are available for the remaining possible orderings: dym(), ymd(),ydm(),mdy(), ormyd()`. For intance: x &lt;- c(&quot;2003, 15-sep&quot;, &quot;1987 24/08&quot;, &quot;1950, Tuesday 25 March&quot;, &quot;19570212&quot;) ydm(x) [1] &quot;2003-09-15&quot; &quot;1987-08-24&quot; &quot;1950-03-25&quot; &quot;1957-12-02&quot; Other functions of this family allow to parse incomplete dates, like year and month or year and quarter providing the date of the first day in the interval: partial &lt;- c(&quot;2003-sep&quot;, &quot;1987.08&quot;, &quot;1950 March&quot;, &quot;1957.2&quot;) # year and month ym(partial) [1] &quot;2003-09-01&quot; &quot;1987-08-01&quot; &quot;1950-03-01&quot; &quot;1957-02-01&quot; partial &lt;- c(&quot;2003-3&quot;, &quot;1987.3&quot;, &quot;1950 1&quot;, &quot;1957.1&quot;) # year and quarter yq(partial) [1] &quot;2003-07-01&quot; &quot;1987-07-01&quot; &quot;1950-01-01&quot; &quot;1957-01-01&quot; Sometimes dates are split in diferent variables for the year, the month and the day. In such a case, a date can be formed with function make_date(), whose arguments are pretty obvious. Note that arguments can be numeric, character, or mixed types: x1 &lt;- &quot;2001&quot; x2 &lt;- 3 x3 &lt;- 5 the_date &lt;- make_date(year = x1, month = x2, day = x3) the_date [1] &quot;2001-03-05&quot; class(the_date) [1] &quot;Date&quot; 2.3.4 Getting date components When components have to be extracted from dates, functions with obvious names are available as shown in the following examples: bd &lt;- ydm(x) # a vector of birth dates bd [1] &quot;2003-09-15&quot; &quot;1987-08-24&quot; &quot;1950-03-25&quot; &quot;1957-12-02&quot; year(bd) # get the year [1] 2003 1987 1950 1957 month(bd) # get the month [1] 9 8 3 12 day(bd) # get the day of month [1] 15 24 25 2 wday(bd) # get the day of the week (1 to 7, starting on sunday) [1] 2 2 7 2   In the case of months and days of the week, argument label = TRUE will provide their names instead of numbers, either abbraviated (default) or full (if argument abbris set to FALSE). Nothe that in this case the result is a ordered factor: month(bd, label=TRUE) [1] Sep Aug Mar Dec 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec wday(bd, label=TRUE, abbr=FALSE) [1] Monday Monday Saturday Monday 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday month(bd, label=TRUE) %&gt;% class() [1] &quot;ordered&quot; &quot;factor&quot; wday(bd, label=TRUE, abbr=FALSE) %&gt;% class() [1] &quot;ordered&quot; &quot;factor&quot; 2.3.5 A closer look to dates Up to now we have seen how dates look when printed to the console, but this is not what the are. In fact, dates are numeric vectors, as the following will reveal: x &lt;- as.Date(c(&quot;2003-09-15&quot;, &quot;1987-08-24&quot;, &quot;1950-03-25&quot;, &quot;1957-12-02&quot;)) xx &lt;- unclass(x) xx [1] 12310 6444 -7222 -4413 class(xx) [1] &quot;numeric&quot; As you see, the actual contents is 12310, 6444, -7222, -4413, and this is the number of days elapsed since an arbitrary origin (set to the first of January 1970). Thus, dates are stored as the number of days elapsed since the origin, as illustrated in figure @ref(fig:dates_internal) (note that dates before the origin are stored as negative numbers). (#fig:dates_internal)Dates as stored in R Package lubridate includes the date-time object origin, so that we can see what’s the origin: as.Date(origin) [1] &quot;1970-01-01&quot;   Now, we can easily verify that the values 12310, 6444, -7222, -4413 stored in date x above are nothing but the numbers of days elapsed since the origin: x - as.Date(origin) Time differences in days [1] 12310 6444 -7222 -4413 Or equivalently, we may add this number of days to the origin and we will get the dates in x. as.Date(origin) + xx [1] &quot;2003-09-15&quot; &quot;1987-08-24&quot; &quot;1950-03-25&quot; &quot;1957-12-02&quot; In summary, a date object is a numeric vector containing whole numbers (the number of days elapsed since the 1st Jan 1970), that have been given the class atribute Date. He is a demo: x &lt;- 1:7 x [1] 1 2 3 4 5 6 7 class(x) [1] &quot;integer&quot; class(x) &lt;- &quot;Date&quot; x [1] &quot;1970-01-02&quot; &quot;1970-01-03&quot; &quot;1970-01-04&quot; &quot;1970-01-05&quot; &quot;1970-01-06&quot; [6] &quot;1970-01-07&quot; &quot;1970-01-08&quot; 2.3.6 Shifting dates Sometimes we may need to create a sequence of dates from a starting point in time. For instance, consider a study in which monthly visits are planned after enrollment of a patient. If we want to evaluate the difference between planned and actual visit dates, we need to create the sequence of planned dates to compare them to actual dates. If visit intervals are defined by a fixed number of days (say, 30 days) we just need to add this number to the enrollment date. enrollment &lt;- as.Date(&quot;2022-01-01&quot;) enrollment [1] &quot;2022-01-01&quot; intervals &lt;- 30 * 0:6 intervals [1] 0 30 60 90 120 150 180 enrollment + intervals [1] &quot;2022-01-01&quot; &quot;2022-01-31&quot; &quot;2022-03-02&quot; &quot;2022-04-01&quot; &quot;2022-05-01&quot; [6] &quot;2022-05-31&quot; &quot;2022-06-30&quot;   Alternatively, we might prefer to set intervals as natural months, but this is not straightforward since the number of days in a month is not constant. Fortunately, lubridate has a family of functions that create natural periods like months, weeks, or years. These functions take the name of the periods they create, and allow to indicate the number of periods want to create as a numeric vector: enrollment + months(0:6) [1] &quot;2022-01-01&quot; &quot;2022-02-01&quot; &quot;2022-03-01&quot; &quot;2022-04-01&quot; &quot;2022-05-01&quot; [6] &quot;2022-06-01&quot; &quot;2022-07-01&quot; enrollment + years(0:5) [1] &quot;2022-01-01&quot; &quot;2023-01-01&quot; &quot;2024-01-01&quot; &quot;2025-01-01&quot; &quot;2026-01-01&quot; [6] &quot;2027-01-01&quot; If the addition of periods results in a non-existing date, these functions will return NA: enrollment &lt;- as.Date(&quot;2001-01-31&quot;) enrollment + months(0:6) [1] &quot;2001-01-31&quot; NA &quot;2001-03-31&quot; NA &quot;2001-05-31&quot; [6] NA &quot;2001-07-31&quot;   To avoid this, the operator %m+% should be used: enrollment %m+% months(0:6) [1] &quot;2001-01-31&quot; &quot;2001-02-28&quot; &quot;2001-03-31&quot; &quot;2001-04-30&quot; &quot;2001-05-31&quot; [6] &quot;2001-06-30&quot; &quot;2001-07-31&quot;   And similarly for leap years: leap_year(2020) [1] TRUE as.Date(&quot;2020-02-29&quot;) + years(0:5) [1] &quot;2020-02-29&quot; NA NA NA &quot;2024-02-29&quot; [6] NA as.Date(&quot;2020-02-29&quot;) %m+% years(0:5) [1] &quot;2020-02-29&quot; &quot;2021-02-28&quot; &quot;2022-02-28&quot; &quot;2023-02-28&quot; &quot;2024-02-29&quot; [6] &quot;2025-02-28&quot; 2.3.7 Intervals Intervals are timespans bounded by start and end dates. Intervals can be created with functioninterval(), or with the operator %--%, from either Date objects or strings in ISO 8601 format: birth &lt;- as.Date(&quot;2001-03-05&quot;) interval(start = birth, end = today()) # from Dates [1] 2001-03-05 UTC--2023-01-18 UTC birth %--% today() # using %--% [1] 2001-03-05 UTC--2023-01-18 UTC &quot;2001-03-05&quot; %--% today() # from ISO 8601 strings [1] 2001-03-05 UTC--2023-01-18 UTC We can use intervals in several ways. For instance, we can use them to compute age at enrollment. To this end, we divide the interval between birth and enrollment dates by a period of one year. If we want to express age in completed years (as we usually do), we truncate the result : birth %--% today() / years(1) # age in years [1] 21.87397 trunc(birth %--% today() / years(1)) # age in complete years [1] 21 Another interesting use of intervals is the detection of overlapping intervals. For instance, we may want to know if a concomitant medication was taken during a specific phase of a clinical trial. This can be done with function int_overlaps(), passing it the two intervals as arguments: treatment_phase &lt;- interval(&quot;2021-01-01&quot;, &quot;2021-03-31&quot;) steroids &lt;- interval(&quot;2020-12-15&quot;, &quot;2020-12-31&quot;) statins &lt;- interval(&quot;2021-02-15&quot;, today()) int_overlaps(treatment_phase, steroids) [1] FALSE int_overlaps(treatment_phase, statins) [1] TRUE Last, intervals can be created also from a sequence of dates stored in a vector, with function int_diff()`. This will create intervals defined by two consecutive visits: visits &lt;- enrollment %m+% months(0:6) visits [1] &quot;2001-01-31&quot; &quot;2001-02-28&quot; &quot;2001-03-31&quot; &quot;2001-04-30&quot; &quot;2001-05-31&quot; [6] &quot;2001-06-30&quot; &quot;2001-07-31&quot; int_diff(visits) [1] 2001-01-31 UTC--2001-02-28 UTC 2001-02-28 UTC--2001-03-31 UTC [3] 2001-03-31 UTC--2001-04-30 UTC 2001-04-30 UTC--2001-05-31 UTC [5] 2001-05-31 UTC--2001-06-30 UTC 2001-06-30 UTC--2001-07-31 UTC 2.4 Dataframe variables In previous sections we have illustrated how to use of several functions to work with strings, factors and dates. To keep the examples code as simple as possible, we illustrated their use on vectors. However, when processing real data, it is very likely that we need to create new variables in a dataframe, which can be achieved with the mutate() function in dplyr. Any of the functions we have discussed can be used within a mutate() statement. For instance, when reading the DISETAHE MS Excel file, the two variables containing dates (data_xtract_dt and ah_dx_dt) are date-time objects in the resulting dataframe: library(readxl) demo &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% select(pid, data_xtract_dt, region, age, sex, ah_dx_dt) class(demo$data_xtract_dt) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; class(demo$ah_dx_dt) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; But when these variables are printed, we see that the time value non-informative (it is always 00:00:00): head(demo) # A tibble: 6 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; 1 11 2003-03-15 00:00:00 6 52 1 1998-01-01 00:00:00 2 15 2003-03-15 00:00:00 6 57 2 1997-01-01 00:00:00 3 20 2003-05-13 00:00:00 1 62 1 2003-04-10 00:00:00 4 24 2003-05-27 00:00:00 10 69 2 1993-01-01 00:00:00 5 33 2003-05-28 00:00:00 10 70 2 1994-01-01 00:00:00 6 37 2003-06-14 00:00:00 9 64 2 1995-04-07 00:00:00   To convert these variables to the simpler and more convenient Date class, the as.Date() function may be used in a mutate() statement: demo %&gt;% mutate(data_xtract_dt = as.Date(data_xtract_dt), ah_dx_dt = as.Date(ah_dx_dt)) # A tibble: 500 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; 1 11 2003-03-15 6 52 1 1998-01-01 2 15 2003-03-15 6 57 2 1997-01-01 3 20 2003-05-13 1 62 1 2003-04-10 4 24 2003-05-27 10 69 2 1993-01-01 5 33 2003-05-28 10 70 2 1994-01-01 6 37 2003-06-14 9 64 2 1995-04-07 7 50 2003-06-16 2 55 2 NA 8 83 2004-03-15 5 64 1 1981-01-01 9 91 2004-03-15 5 57 1 1996-01-01 10 114 2004-04-05 5 73 2 1991-06-04 # … with 490 more rows While this is fine because we need to convert two variables only, it may be cumbersome when many variables have to be converted. 2.4.1 Processing several variables at once When several variables have to be processed the same way, a better approach is to use across() within mutate(). The across() function takes two arguments. This first argument indicates the variables to be processed, and the second argument specifies the function to be applied to these variables: demo %&gt;% mutate(across(c(data_xtract_dt, ah_dx_dt), as.Date)) # A tibble: 500 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; 1 11 2003-03-15 6 52 1 1998-01-01 2 15 2003-03-15 6 57 2 1997-01-01 3 20 2003-05-13 1 62 1 2003-04-10 4 24 2003-05-27 10 69 2 1993-01-01 5 33 2003-05-28 10 70 2 1994-01-01 6 37 2003-06-14 9 64 2 1995-04-07 7 50 2003-06-16 2 55 2 NA 8 83 2004-03-15 5 64 1 1981-01-01 9 91 2004-03-15 5 57 1 1996-01-01 10 114 2004-04-05 5 73 2 1991-06-04 # … with 490 more rows In the first argument, helper functions can be used just like with select(): demo %&gt;% mutate(across(ends_with(&quot;dt&quot;), as.Date)) # A tibble: 500 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; 1 11 2003-03-15 6 52 1 1998-01-01 2 15 2003-03-15 6 57 2 1997-01-01 3 20 2003-05-13 1 62 1 2003-04-10 4 24 2003-05-27 10 69 2 1993-01-01 5 33 2003-05-28 10 70 2 1994-01-01 6 37 2003-06-14 9 64 2 1995-04-07 7 50 2003-06-16 2 55 2 NA 8 83 2004-03-15 5 64 1 1981-01-01 9 91 2004-03-15 5 57 1 1996-01-01 10 114 2004-04-05 5 73 2 1991-06-04 # … with 490 more rows If the function to be applied needs some arguments, they can be passed as further arguments to across(). For instance, suppose we want to define factors for all treatment variables in the DISETAHE study. Because all of them are coded the same way, the factor definition is identical for all: treatments &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% select(pid, lmr:other) treatments %&gt;% mutate(lmr = factor(lmr, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), bb = factor(bb, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), diur = factor(diur, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), acei = factor(acei, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), arb = factor(arb, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), cbb = factor(cbb, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), ab = factor(ab, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;)), other = factor(other, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;))) # A tibble: 500 × 9 pid lmr bb diur acei arb cbb ab other &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 11 yes yes no no no no no no 2 15 yes no no yes no no no no 3 20 yes no no no no no no no 4 24 yes no yes no no no no no 5 33 yes no yes no no no no no 6 37 yes no yes no yes no no no 7 50 no no no no no no no no 8 83 yes no no no no no no yes 9 91 yes no no yes no no no no 10 114 yes no yes no yes no no no # … with 490 more rows Using across(), we need to pass the factor function as second argument, and the levels and labels arguments as further arguments: treatments %&gt;% mutate(across(lmr:other, factor, levels = 1:2, labels = c(&quot;yes&quot;, &quot;no&quot;))) # A tibble: 500 × 9 pid lmr bb diur acei arb cbb ab other &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 11 yes yes no no no no no no 2 15 yes no no yes no no no no 3 20 yes no no no no no no no 4 24 yes no yes no no no no no 5 33 yes no yes no no no no no 6 37 yes no yes no yes no no no 7 50 no no no no no no no no 8 83 yes no no no no no no yes 9 91 yes no no yes no no no no 10 114 yes no yes no yes no no no # … with 490 more rows Sometimes we may want to identify the variables to be processed by their type. For instance, suppose that diagnostic variables had been defined as logical variables, as is the case in dataframe diag below: # A tibble: 500 × 6 pid age sex dx_dm dx_dyslip dx_lvh &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; 1 11 52 1 TRUE TRUE TRUE 2 15 57 2 TRUE TRUE TRUE 3 20 62 1 TRUE TRUE TRUE 4 24 69 2 TRUE FALSE TRUE 5 33 70 2 TRUE FALSE TRUE 6 37 64 2 FALSE TRUE TRUE 7 50 55 2 TRUE NA TRUE 8 83 64 1 TRUE FALSE TRUE 9 91 57 1 TRUE FALSE TRUE 10 114 73 2 FALSE FALSE TRUE # … with 490 more rows Suppose we want to convert them to numeric with as.numeric(). This can be done using where() in the first argument of across() to identify all variables being of logical type, and as.numeric as the second argument: diag %&gt;% mutate(across(where(is.logical), as.numeric)) # A tibble: 500 × 6 pid age sex dx_dm dx_dyslip dx_lvh &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 52 1 1 1 1 2 15 57 2 1 1 1 3 20 62 1 1 1 1 4 24 69 2 1 0 1 5 33 70 2 1 0 1 6 37 64 2 0 1 1 7 50 55 2 1 NA 1 8 83 64 1 1 0 1 9 91 57 1 1 0 1 10 114 73 2 0 0 1 # … with 490 more rows If the operation we need to perform is complex and there is no single function to do it, we can use an expression preceeded by ~, and use .x to indicate where the variable in across is used. For example, supose we want to recode all drug treatments as 1 or 0 instead of 1 or 2 (for yes or no, respectively). This will work: treatments %&gt;% mutate(across(bb:other, ~ ifelse(.x == 1, 1, 0))) # A tibble: 500 × 9 pid lmr bb diur acei arb cbb ab other &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 1 1 0 0 0 0 0 0 2 15 1 0 0 1 0 0 0 0 3 20 1 0 0 0 0 0 0 0 4 24 1 0 1 0 0 0 0 0 5 33 1 0 1 0 0 0 0 0 6 37 1 0 1 0 1 0 0 0 7 50 2 0 0 0 0 0 0 0 8 83 1 0 0 0 0 0 0 1 9 91 1 0 0 1 0 0 0 0 10 114 1 0 1 0 1 0 0 0 # … with 490 more rows Resources For more on strings and regex, look at the vignettes of the stringr package. You may see them by running the following code in the R console: vignette(\"stringr\") vignette(\"regular-expressions\") Of course, there is a stringr R cheat sheet! Try this nice interactive tutorial to practice your regex. For a comparison of stringr and base R functions, see here. Package translateR provides easy access to the Google and Microsoft APIs for languaje detection and translation. If you are interested in fuzzy matching of strings, look at the fuzzywuzzyR and stringdist packages. If you have ICD-9 or ICD-10 codes and need to compute comorbidity indices (such as the Charlson index), have a look to package icd. For more on factors, look at the vignette of the forcats package by running the following code in the R console: vignette(\"forcats\"), or see (this tutorial)[https://www.r-bloggers.com/2020/06/working-with-factors-in-r-tutorial-forcats-package/]. For more on dates and times, see this book chapter, or this comprehensive tutorial. Exercises From the following character vector of NYHA classes, use str_extract() with regular expressions to get two character vectors: one containing roman numerals only, and another one containing the text description, starting with a capital letter (the dots should be removed, as well any leading, trailing or extra white space). nyha &lt;- c(&quot;I. no limitation&quot;, &quot;II. mild symptoms &quot;, &quot;III. Marked symptoms&quot;, &quot;IV. severe limitations&quot;) With the given vector on antecedents of four patients: antec &lt;- c(&quot;Asthma&quot;, &quot;DM and AHT&quot;, &quot;DM, AHT and pneumonia&quot;, &quot;AHT + pneumonia&quot;) Use str_view_all() to test a regular expression detecting all separators between different antecedents of the same patient(and, , or +). Use str_replace_all() to homogenize separators between antecedents using always + (e.g., “DM + AHT + pneumonia”). From the vector you got in the previous question, can you compute a numeric vector showing the number of antecedents for each patient? Use the button to download a dataset on medical antecedents of 21 patients. After reading the data into dataframe d, run the code below to detect all diabetic patients, and then address the following tasks. Download antecedents data d$antecedent %&gt;% str_to_lower() %&gt;% str_view(pattern = &quot;diabet.s|diabetic|dm&quot;) Copy-paste the previous code and adapt the regexp in pattern to detect all patients with arterial hypertension. Do the same to detect all patients with bronchial asthma. Use the previous regexp patterns in str_detect() to create three new logical variables in dataframe d: dm, aht, and ba, as indicators of diabetes mellitus, arterial hypertension and bronchial asthma, respectively. Download the doses data set, read it, and produce the bar chart shown below. Download doses data On the same data, use fct_collapse() to define doses as “low” (25 or 50 mg), “medium” (100 mg) or “high” (150 or 200 mg), and produce the following bar chart: The following code reads the DISETAHE study data, selects drug class variables, verticalises them, and filters patients that took each drug class. From the resulting dataframe drug_classes, reproduce the bar chart shown below (with identical descriptors for drug classes). drug_classes &lt;- readxl::read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% select(pid, bb:other) %&gt;% pivot_longer(bb:other, names_to = &quot;drug_class&quot;, values_to = &quot;value&quot;) %&gt;% filter(value == 1) %&gt;% arrange(pid, drug_class) Create a dataframe with the CIP codes provided below, and use a mutate() statement to compute additional variables sex, dob and age: cip = the CIP code. sex = a factor with labels “male” and “female”. dob = the date of birth; this should be a Date object. age = the patient’s age as of 2022-01-01. cip &lt;- c(&quot;FEGA0720525000&quot;, &quot;ROMA1690913000&quot;, &quot;PEJI0010503&quot;) Repeat the previous exercise after adding this new CIP to the cip vector: “ALCO0580325000”. The date of enrollment of a patient in a study was “2016-01-01”. In this study, follow-up visits should be conducted at 1, 3 and 6 months and 1 year after enrollment, and yearly thereafter up to 5 years. What should be the dates of follow-up visits assuming 30-day months? And what would they be using natural month intervals? Is there any saturday or sunday among them? Use across() in a mutate() statement to recode all yes/no variables in the DISETAHE dataset so that they take values 1 (for yes) or 0 (for no). "],["data-validation.html", "3 Data validation 3.1 Variable types 3.2 Missings 3.3 Ranges 3.4 Codelists 3.5 Uniqueness 3.6 Linear sequences 3.7 Availability of records 3.8 Multivariate rules 3.9 Rules stored in files 3.10 Working with validator objects 3.11 Designing data validation plans (DVP) Resources Exercises", " 3 Data validation The goal of data validation is to ensure that the available data meet certain quality requirements. This is an important step to undertake before proceeding to data analysis. Data problems are commonplace and may be diverse in nature. Examples are non-unique identifiers (patient or visit number), duplicated records, missing or impossible values (like date = “2022-02-30”), or inconsistent values (like age = 87, and menopausal status = “pre-menopausal”). Because these kind of problems are so common in real study datasets, we need to undertake actions to detect and hopoefully fix them. More formally, data validation is defined as an activity aimed at verifying whether or not the values of a variable, or the combination of values of several variables, belong to a pre-specified acceptable set. A data validation plan is a list of all the specific verifications we want to carry out for a clinical study. Each of these verifications is called a validation rule. Table 3.1 shows some examples of validation rules for a hypothetical data set containing variables age, sex and menopausal status, as well as a patient identifier : Table 3.1: Example of validation rules Id Example of validation rules 1 patient identifier is numeric 2 patient identifier is not missing 3 patient identifier is unique 4 age is not missing 5 age value is between 18 and 99 6 sex is not missing 7 sex is either “male” or “female” 8 if sex is “male”, menopausal status is missing 9 if sex is “female”, menopausal status is either “pre-menopausal” or “pos-menopausal” Validation rules can refer to several data characteristics. In table 3.1, rule 1 refers to the type of variable, expected to be numeric; rule 2, 4 and 6 refer to data availability; rule 5 refers to the expected range of values of a quantitative variable; and rules 8 and 9 refer to the expected value of a variable conditional on another variable. These are not the ony types of rules we may possibly define (as you will see in this chapter), but just examples of the most common ones. In table 3.1, rules are stated in plain English. However, for these to be executable on the relevant data, they need to be coded. The R package validate allows to declare data validation rules (such as those in table 3.1), to confront data with them, and to analyze or visualize the results. So, let’s load this package, as well as tidyverse: library(tidyverse) library(validate) In the following sections we show how to implement and execute different types of rules using the validate package. 3.1 Variable types It is worth checking that variables in a dataframe are of the expected type. Suppose we read the DISETAHE data for the first time, and select demographic variables (as defined in section 1.6): library(readxl) demo &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% select(pid:ah_dx_dt) In the resuting dataframe demo, we expect age, sex, and region to be numeric, anddata_xtract_dt and ah_dx_dt to be of class Date. The following script verifies if this is the case. First, we define rules with validator() and save them into an object name of our choice (my_rules): any expression that resolves to a logical value (TRUE or FALSE) can be used as a rule, and different rules are separated by commas; here we use is.numeric() (from base R) and is.Date() from package lubridate, so that we need to load this package previously. Second, we use confront() to execute the rules on dataframe demo, we save the result as res, and print res to the console: library(lubridate) # to use is.Date() my_rules &lt;- validator(is.numeric(age), is.numeric(sex), is.numeric(region), is.Date(data_xtract_dt), is.Date(ah_dx_dt) ) res &lt;- confront(demo, my_rules) res Object of class &#39;validation&#39; Call: confront(dat = demo, x = my_rules) Rules confronted: 5 With fails : 2 With missings: 0 Threw warning: 0 Threw error : 0   The result informs that two of the five rules confronted to demo have failed (With fails). To get more detailed information, we use summary() on the result provided by confront() (res): summary(res) name items passes fails nNA error warning expression 1 V1 1 1 0 0 FALSE FALSE is.numeric(age) 2 V2 1 1 0 0 FALSE FALSE is.numeric(sex) 3 V3 1 1 0 0 FALSE FALSE is.numeric(region) 4 V4 1 0 1 0 FALSE FALSE is.Date(data_xtract_dt) 5 V5 1 0 1 0 FALSE FALSE is.Date(ah_dx_dt)   In the output, rules are identified by names, which by default are V1 to V5, but we could have given custom names when defining the rules in validator(), by preceding each rule with the desired name and an equal sign. For each rule, we are presented with the number of items checked, the number of passes and fails, possible errors or warnings raised when executing the rule and the rule expression. In this case, because all the rules refer to the type of a dataframe variable, only one result is produced per rule, either a pass or a fail, and neither errors nor warnings have been raised. The last two rules have failed, indicating that variables data_xtract_dt and ah_dx_dt are not of the expected class Date. Indeed they are POSIXct objects: class(demo$data_xtract_dt) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; class(demo$ah_dx_dt) [1] &quot;POSIXct&quot; &quot;POSIXt&quot; Let’s fix these by converting to dates and check again. Note the use of the pipe to to confront the rules to demo, and to summarize the result: demo &lt;- demo %&gt;% mutate(across(c(data_xtract_dt, ah_dx_dt), as.Date)) demo %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 1 1 0 0 FALSE FALSE is.numeric(age) 2 V2 1 1 0 0 FALSE FALSE is.numeric(sex) 3 V3 1 1 0 0 FALSE FALSE is.numeric(region) 4 V4 1 1 0 0 FALSE FALSE is.Date(data_xtract_dt) 5 V5 1 1 0 0 FALSE FALSE is.Date(ah_dx_dt) Problem fixed! 3.2 Missings Rules to verify data availability can be defined using !is.na(). Function is.na(x) produces a TRUE when x is missing or a FALSE otherwise. The negation operation ! can be used to reverse the result, so that !is.na(X) checks that x is not missing, resulting in a TRUE when x is available (not missing) and a FALSE when missing. The following checks for availability of demo variables: my_rules &lt;- validator(!is.na(age), !is.na(sex), !is.na(region), !is.na(data_xtract_dt), !is.na(ah_dx_dt) ) demo %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 500 500 0 0 FALSE FALSE !is.na(age) 2 V2 500 491 9 0 FALSE FALSE !is.na(sex) 3 V3 500 500 0 0 FALSE FALSE !is.na(region) 4 V4 500 500 0 0 FALSE FALSE !is.na(data_xtract_dt) 5 V5 500 251 249 0 FALSE FALSE !is.na(ah_dx_dt) In this case, because each rule refers to the values of a variable, they producs as many results as rows in the confronted dataframe (items in the output), 500 in this case. Variables age, region, and data_xtract_dt have no missings since all 500 items passed the rule and none failed. However, nine missings (fails) are observed in sex, and two hundred forty-nine in ah_dx_dt. A visualization of the frequency of fails per rule can be produced using plot() with the result of confront(): demo %&gt;% confront(my_rules) %&gt;% plot() Taking into account that this study was conducted by reviewing medical records, we may think that the date of diagnosis of AHT was not available in the medical records, so that we will have little chances to recover this information. In such a case, we may decide to drop this rule. Let’s then redefine our rules, and take the chance to give them custom names for easier identification of the outputs elements: rules &lt;- validator(age_available = !is.na(age), sex_available = !is.na(sex), region_available = !is.na(region), data_xtraction_available = !is.na(data_xtract_dt) ) demo %&gt;% confront(rules) %&gt;% summary() name items passes fails nNA error warning expression 1 age_available 500 500 0 0 FALSE FALSE !is.na(age) 2 sex_available 500 491 9 0 FALSE FALSE !is.na(sex) 3 region_available 500 500 0 0 FALSE FALSE !is.na(region) 4 data_xtraction_available 500 500 0 0 FALSE FALSE !is.na(data_xtract_dt) demo %&gt;% confront(rules) %&gt;% plot() To identify cases that produced a fail for some rule, function violating() should be used . Note that the result is a dataframe containing all cases that produced a fail in some of the rules: demo %&gt;% violating(rules) # A tibble: 9 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; 1 397 2004-04-30 5 42 NA NA 2 2651 2004-07-06 9 52 NA NA 3 3779 2004-10-06 13 46 NA NA 4 4221 2004-10-27 17 77 NA NA 5 4650 2004-11-24 2 70 NA 1995-01-01 6 5087 2004-12-21 17 71 NA 1994-01-01 7 6195 2005-03-09 8 44 NA NA 8 6232 2005-03-09 14 38 NA NA 9 7154 2005-05-05 1 65 NA NA Sometimes we may want to check for the completeness of a set of important variables, that is, to verify if these variables are all available. This is the job of function is_complete(). Here, we check for rows in demo having complete data on sex, age and region: my_rules &lt;- validator(is_complete(pid, age, sex, region)) confront(demo, my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 500 491 9 0 FALSE FALSE is_complete(pid, age, sex, region) The result is quite obvious in this case, since we already know that there are no missings in age or region, and there are nine missings in sex. But let’s get the rows failing: violating(demo, my_rules) # A tibble: 9 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; 1 397 2004-04-30 5 42 NA NA 2 2651 2004-07-06 9 52 NA NA 3 3779 2004-10-06 13 46 NA NA 4 4221 2004-10-27 17 77 NA NA 5 4650 2004-11-24 2 70 NA 1995-01-01 6 5087 2004-12-21 17 71 NA 1994-01-01 7 6195 2005-03-09 8 44 NA NA 8 6232 2005-03-09 14 38 NA NA 9 7154 2005-05-05 1 65 NA NA Indeed, the nine fails are the same cases reported previously as having sex missing. 3.3 Ranges Checking a range of values is possibly the most common rule imposed on quantitative variables. Range checks are also useful to check if dates are in an expected time window (such as enrollment dates in a study). Range checks can be easily implemented with function in_range(), using its arguments min and max to define the expected range. Here we set minimum and maximum values for age as 18 and 95 respectively. The minimum comes from the selection criteria of the study protocol, but the maximum age is set at 95 for illustration purposes only, since no upper bound for age was set in this study and ages above 95 are perfectly possible. The range defined for the data extraction date (data_xtract_dt) corresponds to the study start and finalization dates. Note the use of the pipes to confront the rules to dataframe demo, and to summarize the result: my_rules &lt;- validator(in_range(age, min = 18, max = 95), in_range(data_xtract_dt, &quot;2004-01-01&quot;, &quot;2005-05-31&quot;)) demo %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 500 499 1 0 FALSE FALSE in_range(age, min = 18, max = 95) 2 V2 500 493 7 0 FALSE FALSE in_range(data_xtract_dt, &quot;2004-01-01&quot;, &quot;2005-05-31&quot;)   The summary shows that one case has an age value out of the expected range, and seven cases fail for the data extraction date. Let’s see what are the cases failing for these two rules: demo %&gt;% violating(my_rules) # A tibble: 8 × 6 pid data_xtract_dt region age sex ah_dx_dt &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; 1 11 2003-03-15 6 52 1 1998-01-01 2 15 2003-03-15 6 57 2 1997-01-01 3 20 2003-05-13 1 62 1 2003-04-10 4 24 2003-05-27 10 69 2 1993-01-01 5 33 2003-05-28 10 70 2 1994-01-01 6 37 2003-06-14 9 64 2 1995-04-07 7 50 2003-06-16 2 55 2 NA 8 562 2004-05-05 3 99 2 1995-01-01 Patient 562 has an age value of 99 years. The remaining patients have a data extraction date previous to the study start in 2004-01-01, and these are possibly data entry errors. Note that, even though sex and region are numeric, we did not check a range for them. The reason is that these can take natural numbers only, like 1 or 2 for sex. A range check would detect unacceptable values like 3, but what about 1.3? A better approach is shown in the next section. 3.4 Codelists A codelist is a list of possible values for a categorical variable. For instance, codes for sex are 1 or 2, and for region are 1, 2, …, 17 (see table 1.1). To verify if all values are in the corresponding codelist we can use the %in% operator followed by a vector with the codelist values: my_rules &lt;- validator(sex %in% 1:2, region %in% 1:17) demo %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 500 491 0 9 FALSE FALSE sex %vin% 1:2 2 V2 500 500 0 0 FALSE FALSE region %vin% 1:17 No fails are detected in this case, meaning that all sex and regionvalues belong to the corresponding codelist. If you look at the expression in the oputput, you will note that the %in% operator has been converted to %vin%. No worries, this is a similar operator that handles `NA``s more conveniently. Codelist rules can be applied to factors as well. Suppose we had already defined factors for sex and region as done here: demo2 &lt;- demo %&gt;% mutate(sex = factor(sex, levels = 1:2, labels = c(&quot;male&quot;, &quot;female&quot;) ), region = factor(region, levels = 1:17, labels = c(&quot;Andalucía&quot;, &quot;Aragón&quot;, &quot;Asturias&quot;, &quot;Baleares&quot;, &quot;Canarias&quot;, &quot;Cantabria&quot;, &quot;Castilla-La Mancha&quot;, &quot;Castilla-León&quot;, &quot;Catalunya&quot;, &quot;Extremadura&quot;, &quot;Galicia&quot;, &quot;La Rioja&quot;, &quot;Madrid&quot;, &quot;Murcia&quot;, &quot;Navarra&quot;, &quot;País Vasco&quot;, &quot;Valencia&quot;))) %&gt;% select(pid, data_xtract_dt, age, sex, region, ah_dx_dt) Then, we should write the rules using appropriate values for these factors, as done below. Some categorical variables have a long codelist. If the codelist is stored somewhere (in a file, in a datframe, or in a vector), you can take advantage of it. Here we illustrate the case where the codelist is already contained in a vector (ccaa): # codelist for region ccaa &lt;- c(&quot;Andalucía&quot;, &quot;Aragón&quot;, &quot;Asturias&quot;, &quot;Baleares&quot;, &quot;Canarias&quot;, &quot;Cantabria&quot;, &quot;Castilla-La Mancha&quot;, &quot;Castilla-León&quot;, &quot;Catalunya&quot;, &quot;Extremadura&quot;, &quot;Galicia&quot;, &quot;La Rioja&quot;, &quot;Madrid&quot;, &quot;Murcia&quot;, &quot;Navarra&quot;, &quot;País Vasco&quot;, &quot;Valencia&quot;) my_rules &lt;- validator(sex %in% c(&quot;male&quot;, &quot;female&quot;), region %in% ccaa) demo2 %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 500 491 0 9 FALSE FALSE sex %vin% c(&quot;male&quot;, &quot;female&quot;) 2 V2 500 500 0 0 FALSE FALSE region %vin% ccaa Codelists are also relevant for categorical variables stored in character vectors. For instance, sex values could have been provided as text (“male” or “female”) rather than as numeric codes (1 or 2). This is possibly the case where codelist rules are most useful, because data entry errors in texts are not uncommon and sometimes difficult to detect (e.g., blank spaces). Suppose that sex and region were recorded as text, as in the following example dataset d with ten patients only: # A tibble: 10 × 6 pid data_xtract_dt age sex region ah_dx_dt &lt;int&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; 1 1 2003-03-15 57 female Cantabria 1997-01-01 2 2 2003-05-13 62 male Andalucía 2003-04-10 3 3 2003-05-27 69 female Extremadura 1993-01-01 4 4 2005-03-02 69 male Castilla-León NA 5 5 2005-03-02 76 male Castilla-León NA 6 6 2005-03-03 76 female Cantabria NA 7 7 2005-03-07 52 female Aragón NA 8 8 2005-03-07 48 female Aragón 1994-07-01 9 9 2005-03-09 72 male Castilla y León NA 10 10 2005-03-09 72 male Castilla y León NA   We can now confront this data with the rules defined above: d %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 10 10 0 0 FALSE FALSE sex %vin% c(&quot;male&quot;, &quot;female&quot;) 2 V2 10 8 2 0 FALSE FALSE region %vin% ccaa Two items failed on the rule for region, let’s see what they are: d %&gt;% violating(my_rules) # A tibble: 2 × 6 pid data_xtract_dt age sex region ah_dx_dt &lt;int&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; 1 9 2005-03-09 72 male Castilla y León NA 2 10 2005-03-09 72 male Castilla y León NA In cases 9 and 10 of d, the region was recorded as “Castilla y León” instead of the expected value “Castilla-León”. 3.5 Uniqueness A critical issue in any data set is that the case identifiers are unique. The violation of uniqueness of identifiers will produce incorrect results when joining tables. In simple tables like demo there is only one identifier, the patient identifier (pid). However, in long-format tables, more identifiers are needed. Consider for instance the structure suggested in table 1.6 to store blood pressure in dataframe bp: bp &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% select(pid, contains(&quot;bp&quot;)) %&gt;% pivot_longer(sbp_v1:dbp_v6, names_to = &quot;variable&quot;) %&gt;% na.omit() %&gt;% separate(variable, into = c(&quot;measure&quot;, &quot;visit&quot;)) %&gt;% pivot_wider(names_from = measure, values_from = value) bp # A tibble: 1,519 × 4 pid visit sbp dbp &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 11 v1 130 80 2 11 v2 140 80 3 15 v1 130 80 4 15 v2 120 80 5 15 v3 120 90 6 15 v4 130 90 7 20 v1 130 85 8 20 v2 120 80 9 20 v3 140 80 10 20 v4 130 80 # … with 1,509 more rows In this case, sbp and dbp are observed variables, while pid and visit are identifiers (also called keys in database jargon). The combination of pid and visit values should be unique in this dataframe, that is, all rows should have a different combination of pid and visit values. The uniqueness of identifiers can be checked with is_unique(), passing it all identifiers as arguments: my_rules &lt;- validator(is_unique(pid, visit)) bp %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 1519 1519 0 0 FALSE FALSE is_unique(pid, visit) In this case, all 1519 rows in bp pass the rule, meaning that the combination of keys are unique (different for all rows). However, consider this example data to see what happens when we apply the same rule: d &lt;- data.frame(pid = c(rep(100:102, each = 3), 100), visit = c(rep(1:3, 3), 3)) d pid visit 1 100 1 2 100 2 3 100 3 4 101 1 5 101 2 6 101 3 7 102 1 8 102 2 9 102 3 10 100 3 d %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 10 8 2 0 FALSE FALSE is_unique(pid, visit) Two fails are found, indicating that there are two rows with the same combination of identifiers. Let’s find them: d %&gt;% violating(my_rules) pid visit 3 100 3 10 100 3 The third and tenth rows both refer to patient 100 and visit 3, an unacceptable duplication of keys. Function all_unique() is a variation of is_unique() producing a single result for the whole dataset confronted: TRUE when all rows have unique combinations of identifiers, or FALSE otherwise: my_rules &lt;- validator(all_unique(pid, visit)) bp %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 1 1 0 0 FALSE FALSE all_unique(pid, visit) d %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 1 0 1 0 FALSE FALSE all_unique(pid, visit) As you see, bp passes, but d fails. 3.6 Linear sequences In many longitudinal studies, patients are assessed repeatedly at pre-specified time points. For instance, suppose a clinical trial where patients are assessed at five different visits, and blood pressure is recorded at each visit. These data can be structured much like the bp dataframe of DISEHTAE, with two keys for patient and visit, and two observed variables for SBP and DBP. We would expect such a dataframe to have five rows per patient, one for each visit. However, if a patient has dropped out or skipped a visit, the sequence of visits for this patient will be incomplete. In the following example dataframe d, the first patient has all visits, but the remaining two patients have an incomplete sequence of visits. # A tibble: 12 × 4 pid visit sbp dbp &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 1 142 80 2 101 2 136 62 3 101 3 150 80 4 101 4 150 78 5 101 5 144 80 6 102 1 160 80 7 102 2 150 80 8 102 3 126 67 9 103 1 140 90 10 103 3 120 70 11 103 4 120 80 12 103 5 100 70 The completeness of a linear sequence can be verified with in_linear_sequence(), by specifying the starting and ending values of the sequence. If the sequence is expected within groups of rows, the variable defining the groups of rows has to be indicated in a by argument. In this case, the sequence is expected within patients, so that we need to specify by = pid: my_rule &lt;- validator(in_linear_sequence(visit, begin = 1, end = 5, by = pid)) d %&gt;% confront(my_rule) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 12 5 7 0 FALSE FALSE in_linear_sequence(visit, begin = 1, end = 5, by = pid) When dataframe d is confronted with the rule, all rows of patients having an incomplete sequence of visits are fails. In this case there are seven such rows, and we list them here: d %&gt;% violating(my_rule) # A tibble: 7 × 4 pid visit sbp dbp &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 102 1 160 80 2 102 2 150 80 3 102 3 126 67 4 103 1 140 90 5 103 3 120 70 6 103 4 120 80 7 103 5 100 70 Indeed, patient 102 has only the first three visits, and patient 103 lacks visit 2. 3.7 Availability of records In the previous section we saw how to detect missing rows in a long dataframe were patients are expected to have a fixed number of rows corresponding to a linear sequence of visits. In other cases however, the fixed structure of rows per patient is not that simple. As an example, consider the rather extreme vertical structure for the same example data shown in the previous section: # A tibble: 24 × 4 pid visit varname value &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 101 1 sbp 142 2 101 1 dbp 80 3 101 2 sbp 136 4 101 2 dbp 62 5 101 3 sbp 150 6 101 3 dbp 80 7 101 4 sbp 150 8 101 4 dbp 78 9 101 5 sbp 144 10 101 5 dbp 80 # … with 14 more rows In this dataframe, pid, visit and varname are key variables, and all observed measures are recorded under value. A patient with complete data should have 5 visits and two variables in each, therefore 5 x 2 = 10 rows. A template of the per-patient structure can be created with expand.grig(), a function of base R that will generate all possible combinations of its arguments: template &lt;- expand.grid(visit = 1:5, varname = c(&quot;sbp&quot;, &quot;dbp&quot;), stringsAsFactors = FALSE) %&gt;% arrange(visit, desc(varname)) template visit varname 1 1 sbp 2 1 dbp 3 2 sbp 4 2 dbp 5 3 sbp 6 3 dbp 7 4 sbp 8 4 dbp 9 5 sbp 10 5 dbp A rule to check the compliance with this structure can be written with contains_exactly(), providing a reference to the template in its keys argument. If the template is to be applied by groups of rows, the grouping variable has to be indicated in the by argument. In this case, the template is to be applied to each and every patient, and thus by = pid. Last, the (per-patient) template needs to be passed to confront() as a named list, the name being the reference we used in the rule definition. my_rule &lt;- validator(contains_exactly(keys = keys_template, by = pid)) d_long %&gt;% confront(my_rule, ref = list(keys_template = template)) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 24 10 14 0 FALSE FALSE contains_exactly(keys = keys_template, by = pid) The result shows 14 fails, corresponding to patients whose rows do not match exactly the template structure of visit and varname combinations. As always, we can use violating() to see the fails: d_long %&gt;% violating(my_rule, ref = list(keys_template = template)) # A tibble: 14 × 4 pid visit varname value &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 102 1 sbp 160 2 102 1 dbp 80 3 102 2 sbp 150 4 102 2 dbp 80 5 102 3 sbp 126 6 102 3 dbp 67 7 103 1 sbp 140 8 103 1 dbp 90 9 103 3 sbp 120 10 103 3 dbp 70 11 103 4 sbp 120 12 103 4 dbp 80 13 103 5 sbp 100 14 103 5 dbp 70 In this case, the deviation from the template structure is due to the lack of visits 4 and 5 in patient 102, and visit 2 in patient 103. However, a case having all visits but a single row for one of them (e.g., sbp, with no row for dbp) would also fail. 3.8 Multivariate rules Multivariate rules are rules that involve more than one variable. When two (or more) variables are related, it might be the case that not all possible combinations of values are acceptable. A particularly common case occurs when a variable is only relevant for certain values of another variable, as is the case in the example rules 8 and 9 of table 3.1. More generally, multivariate rules are useful whenever, for a set of variables, only certain combination of possible values are acceptable. 3.8.1 Inequalities Checking for inequalities is a common type of multivariate rule. For instance, a SBP of 95 mmHg is quite low but not impossible, as is a DBP of 100 mmHg. However, their combination is just impossible since DBP cannot exceed DBP. Therefore, we could check for the inequality SBP &gt; DBP. This is done here for dataframe bp of DISEHTAE: my_rule &lt;- validator(sbp &gt; dbp) bp %&gt;% confront(my_rule) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 1519 1519 0 0 FALSE FALSE sbp &gt; dbp In this case, all 1519 rows in bp pass the rule. Another common application of inequality checks are visit dates in longitudinal studies. Dates are prone to data entry errors, and it is important to verify they comply with the expected chronological order to avoid absurd results, such as negative values, when computing the length of periods. Inequality checks (e.g. date of visit 1 &lt; date of visit 2) help detect inconsistent date sequences. 3.8.2 Conditional restrictions Consider variables glucose and dx_dm in dataframe risk_factors as defined in table 1.6. Non-diabetic patients (dx_dm = \"no\", assuming it was defined as a factor) are expected to have blood glucose values no higher than 126 mg/dl. A rule to check if this is the case can be written using an if statement: a condition is specified within brackets after if, and is followed by a logical expression (i.e., an expression resolving to either TRUE or FALSE): my_rule &lt;- validator(if (dx_dm == &quot;no&quot;) glucose &lt;= 126) risk_factors %&gt;% confront(my_rule) %&gt;% summary() name items passes fails nNA error warning expression 1 V1 500 239 5 256 FALSE FALSE dx_dm != &quot;no&quot; | (glucose - 126 &lt;= 1e-08)   The rule passes in 239 patients, but fails in five, and cannot be evaluated in 256 cases due to missing values in either variable. Let’s see what are the fails: risk_factors %&gt;% violating(my_rule) # A tibble: 5 × 3 pid glucose dx_dm &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; 1 324 137 no 2 1531 157 no 3 1868 168 no 4 2507 131 no 5 3355 184 no While one patient has a glucose value only slightly above 126 mg/dl, the remaining four patients have values well above this limit. Conditional restrictions are very common. A particular case where they are relevant is when a set of variables is recorded conditioned on the value of another variable. The last two rules in table 3.1 exemplify one such case, since menopausal status should be recorded for women only. Other common examples are pregnancy tests conducted in pre-menopausal women only, or adverse event characteristics (such as seriousness, severity, onset date, etc.) that are only recorded when an adverse event is reported. In these and other cases, conditional restrictions help detect inconsistent data such as pregnant men, or patients with no adverse events for whom an event onset date has been recorded. 3.9 Rules stored in files You may have found surprising that validator() does not allow to specify the dataframe we want to check, so that we need to use confront() to specify it. Wouldn’t it be more efficient to solve the problem with a single function? Maybe, but this design is on purpose, and the reason is to allow re-utilization of rules in different studies. All studies record demographic and anthropometric data, all clinical trials record comorbidities, adverse events and concomitant medications, and most record vitals signs, or even ECG and laboratory results. For this reason, clinical research units tend to have standard CRFs and data file formats that are reused in different studies with few study-specific modifications. Standardization makes processes not only more efficient, but also more robust, since repeated utilization will show weaknesses that may be optimized for future uses. Writing validation rules is no exception, and standard rules are worth having. A feature that facilitates the standardization of rules is the possibility of storing them in a file, read this file into a dataframe, and use it to define the rules with validator(). Let’s see how to do it to check some of the demographic variables of DISEHTAE (before defining any factor): demo &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) %&gt;% select(pid, age, sex, region, data_xtract_dt, ah_dx_dt) %&gt;% mutate(across(c(data_xtract_dt, ah_dx_dt), as.Date)) demo # A tibble: 500 × 6 pid age sex region data_xtract_dt ah_dx_dt &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; 1 11 52 1 6 2003-03-15 1998-01-01 2 15 57 2 6 2003-03-15 1997-01-01 3 20 62 1 1 2003-05-13 2003-04-10 4 24 69 2 10 2003-05-27 1993-01-01 5 33 70 2 10 2003-05-28 1994-01-01 6 37 64 2 9 2003-06-14 1995-04-07 7 50 55 2 2 2003-06-16 NA 8 83 64 1 5 2004-03-15 1981-01-01 9 91 57 1 5 2004-03-15 1996-01-01 10 114 73 2 5 2004-04-05 1991-06-04 # … with 490 more rows To check the demo data above, we have defined twelve rules and stored them in a MS Excel file. The following script reads this file and shows its contents: rules_df &lt;- read_excel(&quot;./data/demo_rules.xlsx&quot;) rules_df # A tibble: 12 × 3 name rule description &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 pid_num is.numeric(pid) pid is numeric 2 pid_avail !is.na(pid) pid is not missing 3 pid_unique is_unique(pid) pid is unique 4 age_num is.numeric(age) age is numeric 5 age_avail !is.na(age) age is not missing 6 age_range in_range(age, 18, 95) age range defined in study selection criteria 7 sex_num is.numeric(sex) sex is numeric 8 sex_avail !is.na(sex) sex is not missing 9 sex_codes sex %in% 1:2 sex is either 1 or 2 10 region_num is.numeric(region) region is numeric 11 region_avail !is.na(region) region is not missing 12 region_codes region %in% 1:17 region is a natural number from 1 to 17 The resulting dataframe rules_df can be passed to validator() in its argument .data, provided it has (at least) three variables with the precise names name, rule and description, as shown below: my_rules &lt;- validator(.data = rules_df) demo %&gt;% confront(my_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 pid_num 1 1 0 0 FALSE FALSE is.numeric(pid) 2 pid_avail 500 500 0 0 FALSE FALSE !is.na(pid) 3 pid_unique 500 500 0 0 FALSE FALSE is_unique(pid) 4 age_num 1 1 0 0 FALSE FALSE is.numeric(age) 5 age_avail 500 500 0 0 FALSE FALSE !is.na(age) 6 age_range 500 499 1 0 FALSE FALSE in_range(age, 18, 95) 7 sex_num 1 1 0 0 FALSE FALSE is.numeric(sex) 8 sex_avail 500 491 9 0 FALSE FALSE !is.na(sex) 9 sex_codes 500 491 0 9 FALSE FALSE sex %vin% 1:2 10 region_num 1 1 0 0 FALSE FALSE is.numeric(region) 11 region_avail 500 500 0 0 FALSE FALSE !is.na(region) 12 region_codes 500 500 0 0 FALSE FALSE region %vin% 1:17 3.10 Working with validator objects Function validator() returns objects of a special class validator (this class is defined in package validate): class(my_rules) [1] &quot;validator&quot; attr(,&quot;package&quot;) [1] &quot;validate&quot; Although these objects are not lists (try is.list()), they behave much like lists. In particular, they can be subsetted, which is useful to adapt a particular set of standatd rules to a specific study. For instance, while virtually all clinical studies record age and sex, the region is not always recorded. Then if we were to check demographic variables in a new study with no region variable, we could subset the standard rules as shown below: rules_subset &lt;- my_rules[1:9] demo %&gt;% confront(rules_subset) %&gt;% summary() name items passes fails nNA error warning expression 1 pid_num 1 1 0 0 FALSE FALSE is.numeric(pid) 2 pid_avail 500 500 0 0 FALSE FALSE !is.na(pid) 3 pid_unique 500 500 0 0 FALSE FALSE is_unique(pid) 4 age_num 1 1 0 0 FALSE FALSE is.numeric(age) 5 age_avail 500 500 0 0 FALSE FALSE !is.na(age) 6 age_range 500 499 1 0 FALSE FALSE in_range(age, 18, 95) 7 sex_num 1 1 0 0 FALSE FALSE is.numeric(sex) 8 sex_avail 500 491 9 0 FALSE FALSE !is.na(sex) 9 sex_codes 500 491 0 9 FALSE FALSE sex %vin% 1:2 Similarly, validator objects can be extended. For instance, our standard set of rules for demographic data does not include checks on the date of data extraction, since this is not always recorded. To extend the standard set of rules defined above, we can create a second validator with the new rules, and then add them to rules_subset using the + operator: new_rules &lt;- validator(is.Date(data_xtract_dt), !is.na(data_xtract_dt), in_range(data_xtract_dt, &quot;2004-01-01&quot;, &quot;2005-05-31&quot;)) final_rules &lt;- rules_subset + new_rules demo %&gt;% confront(final_rules) %&gt;% summary() name items passes fails nNA error warning expression 1 pid_num 1 1 0 0 FALSE FALSE is.numeric(pid) 2 pid_avail 500 500 0 0 FALSE FALSE !is.na(pid) 3 pid_unique 500 500 0 0 FALSE FALSE is_unique(pid) 4 age_num 1 1 0 0 FALSE FALSE is.numeric(age) 5 age_avail 500 500 0 0 FALSE FALSE !is.na(age) 6 age_range 500 499 1 0 FALSE FALSE in_range(age, 18, 95) 7 sex_num 1 1 0 0 FALSE FALSE is.numeric(sex) 8 sex_avail 500 491 9 0 FALSE FALSE !is.na(sex) 9 sex_codes 500 491 0 9 FALSE FALSE sex %vin% 1:2 10 V1 1 1 0 0 FALSE FALSE is.Date(data_xtract_dt) 11 V2 500 500 0 0 FALSE FALSE !is.na(data_xtract_dt) 12 V3 500 493 7 0 FALSE FALSE in_range(data_xtract_dt, &quot;2004-01-01&quot;, &quot;2005-05-31&quot;) 3.11 Designing data validation plans (DVP) Designing a useful data validation plan (DVP) is not easy for several reasons. First, because in real studies the number of variables is high, or very high, and the number of potential validation checks grows exponentially with the number of variables (consider the possible number of variable combinations for multivariate rules!); and second, because it is difficult to give general recommendations on what is worth checking, since this will depend on what is the purpose of the DVP, and when are we going to execute it. For instance, in regulatory clinical trials DVPs are very exhaustive and are executed during (or shortly after) data entry, possibly through an electronic CRF, so that the chances to recover accidental missings and amend inconsistent data will be high. However, this will be very different in a small study whose data is received in MS Excel files once the study has been completed. In such a case, the chances to fix data problems will be much lower and at a much higher cost. Consequently, the DVP should concentrate on issues that are either critical, or easily solvable. Because this last case is possibly the most frequent one in many clinical research settings, some advise is given on how to approach the design of the DVP for these studies. The most important recommendation is to refrain from being exhaustive. Achieving perfect data quality is utopic, and attemps to achieve this goal may be extremely time consuming and very likely unsuccessful. Rather, any effort should be made to target issues that are either very important, or easy to fix. Table 3.2 provides some clues on what should be checked according to this attitude, and some clarifications follow. Table 3.2: Recommendations for DVPs in small studies with no eCRF Variable Check for All variables Variable type Key variables Missing or non-unique values Observed variables, critical Missing, implausible or inconsistent values Observed variables, non-critical Impossible values The type of variables should be checked for all variables, and key variables (such as patient, visit and other identifiers) should be checked in all dataframes. One one hand, we cannot afford missing or non-unique identifiers, and on the other hand common problems in these variables are usually easy to fix. Concerning observed variables, it is desirable to concentrate the efforts on critical variables. Reasonable criteria to identify critical variables are: variables needed to assess the study objectives. variables needed to define study populations and exclusions (including study selection criteria). variables needed to characterize the study population (age, sex, etc), or to be described in any communication of the study results. Non-critical variables should be a minority in well designed studies, but this is not always the case. For these variables, only values that are obviously wrong or not credible are worth detecting. When resources are scarce and verification and amendment of non-credible data is not affordable, we can always set them to missing. While not ideal, this is better than proceeding to analysis with gross data errors. Wrong or hardly credible values can be detected with codelist rules for categorical variables and range checks for quantitative variables, but broad limits are recommended in this last case. Setting these limits require expert judgment on what is or is not possible or credible. Easy solutions such as using normal ranges are to be avoided. A last consideration refers to derived variables. Virtually all studies include derived variables that will be reported in papers, such as the body mass index (BMI) computed from body weight and height. In some studies, even primary or secondary outcomes are derived from observed variables, as is the case of the estimated glomerular filtration rate (eGFR) computed from blood creatinine in kidney disease studies. Detecting inconsistencies in the components of a computational formula may be much harder than detecting suspicious values in the results of the computation. For instance, a body weight of 48 kg is perfectly possible, as is a body height of 182 cm, but their combination is unlikely. To devise a rule that detects this type of inconsistency is not easy. However, the BMI of 14.5 kg/m^2 resulting from the previous values is highly suspicious and can be easily detected by a range check. For this reason, it is recommended to include derived variables in the DVP, and compute them before confronting the rules to the data. Resources For more on the validate package, see The Data Validation Cookbook. If you need to deal with ICD-9 or ICD-10 codes you may want to explore the R package icd, or look at this short intro. Work with medical immages? Have a look at this CRAN Task View. If your raw data is stored in a database, you want to look at this CRAN Task View. Though primarily focussed on data analysis, you may want to know about CRAN Task Views for Clinical Trials, Pharmacokinetic Data Analysis, Psychometrics, and Genetics. Exercises Run the following code to create the dataframes defined in table 1.6, with no factors created. library(readxl) ah &lt;- read_excel(&quot;./data/hta.xlsx&quot;, sheet = &quot;data&quot;) demo &lt;- ah %&gt;% select(pid:ah_dx_dt) %&gt;% mutate(across(c(data_xtract_dt, ah_dx_dt), as.Date)) risk_factors &lt;- ah %&gt;% select(pid, glucose:creatinine) treatments &lt;- ah %&gt;% select(pid, lmr:other) bp &lt;- ah %&gt;% select(pid, contains(&quot;bp&quot;)) %&gt;% pivot_longer(sbp_v1:dbp_v6, names_to = &quot;variable&quot;) %&gt;% na.omit() %&gt;% separate(variable, into = c(&quot;measure&quot;, &quot;visit&quot;)) %&gt;% pivot_wider(names_from = measure, values_from = value) rm(ah) Create a validator object to check that the patient identifier is numeric, has no missings, and is unique. Then confront this object with all dataframes, exception made of bp. In dataset bp, the visit is a character variable taking values \"v1\", \"v2\", .... Convert it to a numeric variable (taking values 1, 2, …), and then, create a validator object to check that the keys of bp are numeric, do not have missings, and visit numbers are always natural numbers in the range 1 to 6. Produce a plot showing the distribution of missing for all observed variables in dataframe risk_factors. Verify that all categorical variables in risk_factors have values in the corresponding codelist (see coding in table 1.1). Verify that patients without a diagnosis of dyslipidemia have a value of total cholesterol below 240 mg/dl (acording to guidelines by the time the study was conducted). In many studies, demographic and anthropometric data are measured at baseline and collected in a demo dataframe. Common variables in such a dataframe are age, sex, weight, and height, as well as a patient identifier. Prepare such a dataframe from the DISEHTAE data, and create a validator object with all rules deemed necessary to control for the completness and quality of data. Assume the study was conducted in adults of either sex, age is no lower than 18y and no greater than 99y, and the BMI is between 16 and 40 kg/m2. Save the rules defined in the validator object you created in the previous exercise to an external file. Hint: use as.data.frame() with the validator object, and then function rio::export() to export to file “demo_rules.xlsx”. Read the Ecxel data file you just created, and use it to define a validator object and to confront the following new demographic data file. Identify any patient violating a rule. Download new demo data "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
